{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7ih7e5O6rX_",
        "tags": []
      },
      "source": [
        "# Running InstructLab with a GPU\n",
        "\n",
        "<ul>\n",
        "<li>Contributors: InstructLab team and IBM Research Technology Education team:\n",
        "<li>Questions and support: kochel@us.ibm.com, IBM.Research.JupyterLab@ibm.com\n",
        "<li>Version: 1.0.13\n",
        "<li>Release date: 2025-03-20\n",
        "<li>Compute requirements: Colab with GPU\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ics9GgZ-6rYB",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "# Summary\n",
        "This Jupyter notebook demonstrates InstructLab, an open source AI project that facilitates knowledge and skills contributions to Large Language Models (LLMs). InstructLab uses a novel synthetic data-based alignment tuning method for Large Language Models introduced in this [paper](https://arxiv.org/abs/2403.01081). The open source InstructLab repository is available [here](https://github.com/instructlab/instructlab) and provides additional documentation on using InstructLab.\n",
        "\n",
        "The InstructLab method consists of three major components:\n",
        "* **Taxonomy-driven data curation:**  The taxonomy is a set of training data curated by humans as examples of new knowledge and skills for the model.\n",
        "* **Large-scale synthetic data generation:** A teacher model is used to generate new examples based on the seed training data. Since synthetic data can vary in quality, InstructLab adds an automated step to refine the example answers, ensuring they are grounded and safe.\n",
        "* **Iterative model alignment tuning:** The model is retrained based on the synthetic data. The InstructLab method includes two tuning phases: knowledge tuning, followed by skill tuning.\n",
        "\n",
        "<img src=\"https://github.com/KenOcheltree/ilab-colab/blob/main/data/images/Flow.png?raw=1\" width=\"800\">\n",
        "\n",
        "InstructLab can take the form of an open source installation or a Red Hat AI InstructLab installation. In this notebook, we will demonstrate the open source version of InstructLab running on Colab with a GPU, broken into the following major sequential sections:\n",
        "* Configuring InstructLab\n",
        "* Generating Syntehtic Data\n",
        "* Training with InstructLab\n",
        "* Inferencing with InstructLab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJjDTcrmAPVS"
      },
      "source": [
        "# Running this Notebook\n",
        "\n",
        "**IMPORTANT:** This notebook must be run within a Colab GPU runtime. You can check you are running with a GPU by selecting Runtime-> Change Runtime Type and confirming that a GPU Runtime is selected. While this notebook can be started on a free Colab account, the GPUs availabe with a free access do not have sufficient memory to run InstructLab training.\n",
        "\n",
        "You can run this notebook either:\n",
        "- Running All Cells by selecting Runtime->Run all\n",
        "- Cell by cell by selecting the arrow on each code cell and running them sequentially.\n",
        "\n",
        "Once the Configuring Instructlab section has been run, the other sections of this notebook can be repeatedly run on other data sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlk9FjFB6rYD",
        "tags": []
      },
      "source": [
        "# Section 1. Configure InstructLab\n",
        "\n",
        "## Step 1.1 Environment Configuration\n",
        "Replicate the ilab data repository containing the pip requirements and data files and run the pip installs that require a reset.\n",
        "\n",
        "**IMPORTANT:** Run the next cell, allow it to complete running, then Restart the session , run the following cell to specify parameters and then you can run the remainder of the notebook.\n",
        "\n",
        "After selecting parameters, the remainder of this notebook can be run either:\n",
        "- Running All Cells by selecting Runtime->Run cell and below\n",
        "- Cell by cell by selecting the arrow on each code cell and running them sequentially.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rde52-02CfiL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86d245fa-d2e1-4a36-eaca-ece972d52a19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.11/dist-packages (2.5.1)\n",
            "Requirement already satisfied: psutil==7.0.0 in /usr/local/lib/python3.11/dist-packages (7.0.0)\n",
            "Requirement already satisfied: pillow==10.4.0 in /usr/local/lib/python3.11/dist-packages (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.5.1) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "# Run this cell, then perform the requested Reset\n",
        "import os\n",
        "if not os.path.exists(\"ilab\"):\n",
        "    !git clone https://github.com/KenOcheltree/ilab.git\n",
        "!pip install numpy==1.26.4 torch==2.5.1 psutil==7.0.0 pillow==10.4.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dmB_IVBPkZ1",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "## Step 1.2 Optionally, provide your own InstructLab QNA data set\n",
        "\n",
        "You can optionally provide your own InstructLab QNA file for processing in this step.\n",
        "\n",
        "**Note:** You may want to run this notebook with an existing dataset before creating your own to understand the InstructLab flow.\n",
        "\n",
        "Follow these steps to add your own dataset:\n",
        "1. Create your own qna.yaml file following the directions on the InstructLab taxonomy [readme](https://github.com/instructlab/taxonomy).\n",
        "1. Create a questions.txt file with related sample questions to use on inferencing.\n",
        "1. Add your qna.yaml and sample questions.txt files to the /content/ilab/data/your_content_1 folder or the /content/ilab/data/your_content_2 folder by dragging and dropping them in the desired folder.\n",
        "1. Double click on the /content/ilab/config.json file to edit and specify the qna_location where your data resides within the Dewey Decimal classification system. Close and save the config.json file.\n",
        "1. You can now specify to run with your own data by selecting **Your Content 1** or **Your Content 2** in the next code cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV_SZDBZgMVa"
      },
      "source": [
        "## Step 1.3 Select InstructLab Parameters\n",
        "Run this next cell, select the following parameters, then follow the direction in the next text cell to run the notebook.\n",
        "\n",
        "We've provided question-and-answer files for these datasets: \"2024 Oscar Awards Ceremony\" and \"Quantum Roadmap and Patterns\" and \"Artificial Intelligence Agents\". Feel free to choose one of these datasets, or select your own custom dataset in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "9E0Z6oO2L_3I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378,
          "referenced_widgets": [
            "57f5e5ba598947a1aee2c57f44ebceec",
            "74d7ca3c5f784184b18207d5771a5ef3",
            "cd1f8b1a24e04357b08f52684e4548a0",
            "19b65567f6d94183bf83daad67717d6d",
            "b677242cc4a14a38a5b4cbac8cc20ed6",
            "1452a8ed170c49e9bd09fdfb71f294d1",
            "def5a6389bae44b9a8c5b803611aaf56",
            "c493ff0794ec4e838f5c9dcaf0daf978",
            "78d76c42014844aaae51d19751db7a43",
            "787427a67bc542229610bd8707dbcee6",
            "f7dc20f8da87426cb00e35cd8e554c58",
            "d8b32b5e04734512899fdf2852b8b585",
            "54ef6b453a14499cab3afb8b54f566df",
            "03aa70a8d4c74172bf894021614d3d74",
            "6630967c096c4a74b5c0afd5056520dc",
            "b1b0514af67e4d2ca64704069932a337",
            "001065ea967d437ab042e77be47c3fae",
            "980ac5b3932f445f97071174eecc5741"
          ]
        },
        "outputId": "3721f71a-5db3-4976-e29f-30f6febb338a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Select the Dataset for this run:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'cybersecurity'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Select the Synthetic Data Generation parameter to use:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ToggleButtons(description='Processing:', options=('Simple', 'Full with GPU'), style=ToggleButtonsStyle(button_…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57f5e5ba598947a1aee2c57f44ebceec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ToggleButtons(description='# of QNAs:', options=('Default (>450)', '>15', '>50', '>200', '>500', '>1000'), sty…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "19b65567f6d94183bf83daad67717d6d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Select the Training parameters to use:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ToggleButtons(description='Epochs:', index=2, options=('1', '2', '3', '4', '5', '10', '15'), style=ToggleButto…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "def5a6389bae44b9a8c5b803611aaf56"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ToggleButtons(description='Iterations:', index=2, options=('1', '3', '5', '10', '20', '50', '100', '200'), sty…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "787427a67bc542229610bd8707dbcee6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Select what to do with the model after training:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ToggleButtons(description='Live Q&A:', options=('Yes', 'No'), style=ToggleButtonsStyle(button_width='auto'), v…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "54ef6b453a14499cab3afb8b54f566df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ToggleButtons(description='Download:', index=1, options=('Yes', 'No'), style=ToggleButtonsStyle(button_width='…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1b0514af67e4d2ca64704069932a337"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After selecting the parameters, select the next cell and then choose Runtime->Run cell and below\n",
            "When that run completes, you can come here, choose different parameters and rerun at the next cell with Runtime->Run cell and below\n",
            "Note: You can also go back and rerun individual sections of the notebook with different parameters.\n"
          ]
        }
      ],
      "source": [
        "# Run this second cell to show parameters\n",
        "import ipywidgets as widgets\n",
        "#See instructions on placing your hf_token in colab userdata\n",
        "from google.colab import userdata\n",
        "hf_token=userdata.get('hf_token')\n",
        "data_set = widgets.ToggleButtons(\n",
        "    options=['cybersecurity'],\n",
        "    description='Dataset:', disabled=False, button_style='', style={\"button_width\": \"auto\"}\n",
        ")\n",
        "data_set = \"cybersecurity\"\n",
        "sdg_pipe = widgets.ToggleButtons(\n",
        "    options=['Simple', 'Full with GPU'],\n",
        "    description='Processing:', disabled=False, button_style='', style={\"button_width\": \"auto\"}\n",
        ")\n",
        "instr=widgets.ToggleButtons(\n",
        "    options=['Default (>450)','>15', '>50', '>200', '>500', '>1000'],\n",
        "    description='# of QNAs:', disabled=False, button_style='', style={\"button_width\": \"auto\"}\n",
        ")\n",
        "train_pipe = widgets.ToggleButtons(\n",
        "    options=['Simple with GPU','Accelerated GPU'],\n",
        "    description='Processing', disabled=False, button_style='', style={\"button_width\": \"auto\"}\n",
        ")\n",
        "epoch=widgets.ToggleButtons(\n",
        "    options=['1', '2', '3', '4', '5', '10', '15'],\n",
        "    description='Epochs:', disabled=False, button_style='', style={\"button_width\": \"auto\"}\n",
        ")\n",
        "it=widgets.ToggleButtons(\n",
        "    options=['1', '3', '5','10','20','50','100','200'],\n",
        "    description='Iterations:', disabled=False, button_style='', style={\"button_width\": \"auto\"}\n",
        ")\n",
        "questions=widgets.ToggleButtons(\n",
        "    options=['Yes','No'],\n",
        "    description='Live Q&A:', disabled=False, button_style='', style={\"button_width\": \"auto\"}\n",
        ")\n",
        "download=widgets.ToggleButtons(\n",
        "    options=['Yes','No'],\n",
        "    description='Download:', disabled=False, button_style='', style={\"button_width\": \"auto\"}\n",
        ")\n",
        "print(\"\\nSelect the Dataset for this run:\")\n",
        "display(data_set)\n",
        "print(\"Select the Synthetic Data Generation parameter to use:\")\n",
        "sdg_pipe.value='Simple'\n",
        "display(sdg_pipe)\n",
        "instr.value = 'Default (>450)'\n",
        "display(instr)\n",
        "print(\"Select the Training parameters to use:\")\n",
        "train_pipe.value='Simple with GPU'\n",
        "#display(train_pipe)\n",
        "epoch.value=\"3\"\n",
        "display(epoch)\n",
        "it.value=\"5\"\n",
        "display(it)\n",
        "print(\"Select what to do with the model after training:\")\n",
        "questions.value=\"Yes\"\n",
        "display(questions)\n",
        "download.value=\"No\"\n",
        "display(download)\n",
        "print(\"After selecting the parameters, select the next cell and then choose Runtime->Run cell and below\")\n",
        "print(\"When that run completes, you can come here, choose different parameters and rerun at the next cell with Runtime->Run cell and below\")\n",
        "print(\"Note: You can also go back and rerun individual sections of the notebook with different parameters.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVAfX3Q5CfiL",
        "tags": []
      },
      "source": [
        "## 1.4 Complete Environment Set Up and Optionally Run All\n",
        "This next code cell installs the remainder of the reuired pip packages and takes about 7 minutes to run.\n",
        "\n",
        "If you perform **Runtime->Run cell and below** on this cell, the rest of notebook will take about an hour to run. After running, it will present a prompt for providing questions to the pre-trained and trained models to test improvements in the model.\n",
        "\n",
        "**Note:** Please ignore the pip dependency errors that appear in the output of the pip installs. They do not affect the successful running of Instructlab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "aFtTdSAbCfiL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "58ce2782-733a-43cc-ecbd-e1e3513e4d9a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: No matching packages for pattern \"llama_cpp_python\"\u001b[0m\u001b[33m\n",
            "\u001b[0mFiles removed: 0\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'ilab/requirements_gpu.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0mPackage                            Version\n",
            "---------------------------------- -------------------\n",
            "absl-py                            1.4.0\n",
            "accelerate                         1.0.1\n",
            "aiofiles                           24.1.0\n",
            "aiohappyeyeballs                   2.6.1\n",
            "aiohttp                            3.11.14\n",
            "aiosignal                          1.3.2\n",
            "airportsdata                       20250224\n",
            "alabaster                          1.0.0\n",
            "albucore                           0.0.23\n",
            "albumentations                     2.0.5\n",
            "ale-py                             0.10.2\n",
            "altair                             5.5.0\n",
            "annotated-types                    0.7.0\n",
            "anyio                              4.9.0\n",
            "appdirs                            1.4.4\n",
            "argon2-cffi                        23.1.0\n",
            "argon2-cffi-bindings               21.2.0\n",
            "array_record                       0.7.1\n",
            "arviz                              0.21.0\n",
            "astor                              0.8.1\n",
            "astropy                            7.0.1\n",
            "astropy-iers-data                  0.2025.3.24.0.35.32\n",
            "astunparse                         1.6.3\n",
            "atpublic                           5.1\n",
            "attrs                              25.3.0\n",
            "audioread                          3.0.1\n",
            "autograd                           1.7.0\n",
            "babel                              2.17.0\n",
            "backcall                           0.2.0\n",
            "backoff                            2.2.1\n",
            "beautifulsoup4                     4.13.3\n",
            "betterproto                        2.0.0b6\n",
            "bigframes                          1.42.0\n",
            "bigquery-magics                    0.9.0\n",
            "bitsandbytes                       0.45.4\n",
            "blake3                             1.0.4\n",
            "bleach                             6.2.0\n",
            "blinker                            1.9.0\n",
            "blis                               1.2.0\n",
            "blosc2                             3.2.1\n",
            "bokeh                              3.6.3\n",
            "boto3                              1.37.24\n",
            "botocore                           1.37.24\n",
            "Bottleneck                         1.4.2\n",
            "bqplot                             0.12.44\n",
            "branca                             0.8.1\n",
            "CacheControl                       0.14.2\n",
            "cachetools                         5.5.2\n",
            "catalogue                          2.0.10\n",
            "certifi                            2025.1.31\n",
            "cffi                               1.17.1\n",
            "chardet                            5.2.0\n",
            "charset-normalizer                 3.4.1\n",
            "chex                               0.1.89\n",
            "clarabel                           0.10.0\n",
            "click                              8.1.8\n",
            "click-didyoumean                   0.3.1\n",
            "cloudpathlib                       0.21.0\n",
            "cloudpickle                        3.1.1\n",
            "cmake                              3.31.6\n",
            "cmdstanpy                          1.2.5\n",
            "colorama                           0.4.6\n",
            "colorcet                           3.1.0\n",
            "colorlover                         0.3.0\n",
            "colour                             0.1.5\n",
            "community                          1.0.0b1\n",
            "compressed-tensors                 0.9.1\n",
            "confection                         0.1.5\n",
            "cons                               0.4.6\n",
            "contourpy                          1.3.1\n",
            "cramjam                            2.9.1\n",
            "cryptography                       43.0.3\n",
            "cuda-python                        12.6.2.post1\n",
            "cudf-cu12                          25.2.1\n",
            "cudf-polars-cu12                   25.2.2\n",
            "cufflinks                          0.17.3\n",
            "cuml-cu12                          25.2.1\n",
            "cupy-cuda12x                       13.3.0\n",
            "cuvs-cu12                          25.2.1\n",
            "cvxopt                             1.3.2\n",
            "cvxpy                              1.6.4\n",
            "cycler                             0.12.1\n",
            "cyipopt                            1.5.0\n",
            "cymem                              2.0.11\n",
            "Cython                             3.0.12\n",
            "dask                               2024.12.1\n",
            "dask-cuda                          25.2.0\n",
            "dask-cudf-cu12                     25.2.2\n",
            "dask-expr                          1.1.21\n",
            "dataclasses-json                   0.6.7\n",
            "DataProperty                       1.1.0\n",
            "datascience                        0.17.6\n",
            "datasets                           2.21.0\n",
            "db-dtypes                          1.4.2\n",
            "dbus-python                        1.2.18\n",
            "debugpy                            1.8.0\n",
            "decorator                          4.4.2\n",
            "deepspeed                          0.16.5\n",
            "defusedxml                         0.7.1\n",
            "Deprecated                         1.2.18\n",
            "depyf                              0.18.0\n",
            "diffusers                          0.32.2\n",
            "dill                               0.3.8\n",
            "diskcache                          5.6.3\n",
            "distributed                        2024.12.1\n",
            "distributed-ucxx-cu12              0.42.0\n",
            "distro                             1.9.0\n",
            "dlib                               19.24.6\n",
            "dm-tree                            0.1.9\n",
            "dnspython                          2.7.0\n",
            "docker-pycreds                     0.4.0\n",
            "docling                            2.28.4\n",
            "docling-core                       2.25.0\n",
            "docling-ibm-models                 3.4.1\n",
            "docling-parse                      4.0.0\n",
            "docstring_parser                   0.16\n",
            "docutils                           0.21.2\n",
            "dopamine_rl                        4.1.2\n",
            "duckdb                             1.2.1\n",
            "earthengine-api                    1.5.8\n",
            "easydict                           1.13\n",
            "easyocr                            1.7.2\n",
            "editdistance                       0.8.1\n",
            "eerepr                             0.1.1\n",
            "einops                             0.8.1\n",
            "email_validator                    2.2.0\n",
            "en_core_web_sm                     3.8.0\n",
            "entrypoints                        0.4\n",
            "et_xmlfile                         2.0.0\n",
            "etils                              1.12.2\n",
            "etuples                            0.3.9\n",
            "evaluate                           0.4.3\n",
            "Farama-Notifications               0.0.4\n",
            "fastai                             2.7.19\n",
            "fastapi                            0.115.12\n",
            "fastapi-cli                        0.0.7\n",
            "fastcore                           1.7.29\n",
            "fastdownload                       0.0.7\n",
            "fastjsonschema                     2.21.1\n",
            "fastprogress                       1.0.3\n",
            "fastrlock                          0.8.3\n",
            "filelock                           3.18.0\n",
            "filetype                           1.2.0\n",
            "firebase-admin                     6.7.0\n",
            "flash_attn                         2.7.4.post1\n",
            "Flask                              3.1.0\n",
            "flatbuffers                        25.2.10\n",
            "flax                               0.10.4\n",
            "folium                             0.19.5\n",
            "fonttools                          4.56.0\n",
            "frozendict                         2.4.6\n",
            "frozenlist                         1.5.0\n",
            "fsspec                             2024.6.1\n",
            "future                             1.0.0\n",
            "gast                               0.6.0\n",
            "gcsfs                              2025.3.0\n",
            "GDAL                               3.6.4\n",
            "gdown                              5.2.0\n",
            "geemap                             0.35.3\n",
            "geocoder                           1.38.1\n",
            "geographiclib                      2.0\n",
            "geopandas                          1.0.1\n",
            "geopy                              2.4.1\n",
            "gguf                               0.10.0\n",
            "gin-config                         0.5.0\n",
            "gitdb                              4.0.12\n",
            "GitPython                          3.1.44\n",
            "glob2                              0.7\n",
            "google                             2.0.3\n",
            "google-ai-generativelanguage       0.6.15\n",
            "google-api-core                    2.24.2\n",
            "google-api-python-client           2.164.0\n",
            "google-auth                        2.38.0\n",
            "google-auth-httplib2               0.2.0\n",
            "google-auth-oauthlib               1.2.1\n",
            "google-cloud-aiplatform            1.86.0\n",
            "google-cloud-bigquery              3.31.0\n",
            "google-cloud-bigquery-connection   1.18.2\n",
            "google-cloud-bigquery-storage      2.30.0\n",
            "google-cloud-bigtable              2.30.0\n",
            "google-cloud-core                  2.4.3\n",
            "google-cloud-dataproc              5.18.1\n",
            "google-cloud-datastore             2.20.2\n",
            "google-cloud-firestore             2.20.1\n",
            "google-cloud-functions             1.20.2\n",
            "google-cloud-iam                   2.18.3\n",
            "google-cloud-language              2.17.1\n",
            "google-cloud-pubsub                2.29.0\n",
            "google-cloud-resource-manager      1.14.2\n",
            "google-cloud-spanner               3.53.0\n",
            "google-cloud-storage               2.19.0\n",
            "google-cloud-translate             3.20.2\n",
            "google-colab                       1.0.0\n",
            "google-crc32c                      1.7.1\n",
            "google-genai                       1.8.0\n",
            "google-generativeai                0.8.4\n",
            "google-pasta                       0.2.0\n",
            "google-resumable-media             2.7.2\n",
            "google-spark-connect               0.5.2\n",
            "googleapis-common-protos           1.69.2\n",
            "googledrivedownloader              1.1.0\n",
            "graphviz                           0.20.3\n",
            "greenlet                           3.1.1\n",
            "grpc-google-iam-v1                 0.14.2\n",
            "grpc-interceptor                   0.15.4\n",
            "grpcio                             1.71.0\n",
            "grpcio-status                      1.71.0\n",
            "grpclib                            0.4.7\n",
            "gspread                            6.2.0\n",
            "gspread-dataframe                  4.0.0\n",
            "gym                                0.25.2\n",
            "gym-notices                        0.0.8\n",
            "gymnasium                          1.1.1\n",
            "h11                                0.14.0\n",
            "h2                                 4.2.0\n",
            "h5netcdf                           1.6.1\n",
            "h5py                               3.13.0\n",
            "haystack-ai                        2.11.2\n",
            "haystack-experimental              0.8.0\n",
            "hdbscan                            0.8.40\n",
            "hf_transfer                        0.1.9\n",
            "highspy                            1.9.0\n",
            "hjson                              3.1.0\n",
            "holidays                           0.69\n",
            "holoviews                          1.20.2\n",
            "hpack                              4.1.0\n",
            "html5lib                           1.1\n",
            "httpcore                           1.0.7\n",
            "httpimport                         1.4.1\n",
            "httplib2                           0.22.0\n",
            "httptools                          0.6.4\n",
            "httpx                              0.28.1\n",
            "httpx-sse                          0.4.0\n",
            "huggingface-hub                    0.29.3\n",
            "humanize                           4.12.2\n",
            "hyperframe                         6.1.0\n",
            "hyperopt                           0.2.7\n",
            "ibis-framework                     9.5.0\n",
            "idna                               3.10\n",
            "imageio                            2.37.0\n",
            "imageio-ffmpeg                     0.6.0\n",
            "imagesize                          1.4.1\n",
            "imbalanced-learn                   0.13.0\n",
            "immutabledict                      4.2.1\n",
            "importlib_metadata                 8.6.1\n",
            "importlib_resources                6.5.2\n",
            "imutils                            0.5.4\n",
            "inflect                            7.5.0\n",
            "iniconfig                          2.1.0\n",
            "instructlab                        0.24.3\n",
            "instructlab-dolomite               0.2.0\n",
            "instructlab-eval                   0.5.1\n",
            "instructlab-quantize               0.1.0\n",
            "instructlab-schema                 0.4.2\n",
            "instructlab-sdg                    0.7.3\n",
            "instructlab-training               0.7.0\n",
            "intel-cmplr-lib-ur                 2025.1.0\n",
            "intel-openmp                       2025.1.0\n",
            "interegular                        0.3.3\n",
            "ipyevents                          2.0.2\n",
            "ipyfilechooser                     0.6.0\n",
            "ipykernel                          6.17.1\n",
            "ipyleaflet                         0.19.2\n",
            "ipyparallel                        8.8.0\n",
            "ipython                            7.34.0\n",
            "ipython-genutils                   0.2.0\n",
            "ipython-sql                        0.5.0\n",
            "ipytree                            0.2.2\n",
            "ipywidgets                         7.7.1\n",
            "itsdangerous                       2.2.0\n",
            "jax                                0.5.2\n",
            "jax-cuda12-pjrt                    0.5.1\n",
            "jax-cuda12-plugin                  0.5.1\n",
            "jaxlib                             0.5.1\n",
            "jeepney                            0.7.1\n",
            "jellyfish                          1.1.0\n",
            "jieba                              0.42.1\n",
            "Jinja2                             3.1.6\n",
            "jiter                              0.9.0\n",
            "jmespath                           1.0.1\n",
            "joblib                             1.4.2\n",
            "jsonlines                          3.1.0\n",
            "jsonpatch                          1.33\n",
            "jsonpickle                         4.0.2\n",
            "jsonpointer                        3.0.0\n",
            "jsonref                            1.1.0\n",
            "jsonschema                         4.23.0\n",
            "jsonschema-specifications          2024.10.1\n",
            "jupyter-client                     6.1.12\n",
            "jupyter-console                    6.1.0\n",
            "jupyter_core                       5.7.2\n",
            "jupyter-leaflet                    0.19.2\n",
            "jupyter-server                     1.16.0\n",
            "jupyterlab_pygments                0.3.0\n",
            "jupyterlab_widgets                 3.0.13\n",
            "kaggle                             1.7.4.2\n",
            "kagglehub                          0.3.10\n",
            "keras                              3.8.0\n",
            "keras-hub                          0.18.1\n",
            "keras-nlp                          0.18.1\n",
            "keyring                            23.5.0\n",
            "kiwisolver                         1.4.8\n",
            "langchain                          0.3.21\n",
            "langchain-community                0.3.20\n",
            "langchain-core                     0.3.49\n",
            "langchain-openai                   0.3.11\n",
            "langchain-text-splitters           0.3.7\n",
            "langcodes                          3.5.0\n",
            "langsmith                          0.3.19\n",
            "language_data                      1.3.0\n",
            "lark                               1.2.2\n",
            "latex2mathml                       3.77.0\n",
            "launchpadlib                       1.10.16\n",
            "lazr.restfulclient                 0.14.4\n",
            "lazr.uri                           1.0.6\n",
            "lazy_imports                       0.4.0\n",
            "lazy_loader                        0.4\n",
            "libclang                           18.1.1\n",
            "libcudf-cu12                       25.2.1\n",
            "libcugraph-cu12                    25.2.0\n",
            "libcuml-cu12                       25.2.1\n",
            "libcuvs-cu12                       25.2.1\n",
            "libkvikio-cu12                     25.2.1\n",
            "libraft-cu12                       25.2.0\n",
            "librosa                            0.11.0\n",
            "libucx-cu12                        1.18.0\n",
            "libucxx-cu12                       0.42.0\n",
            "lightgbm                           4.5.0\n",
            "linkify-it-py                      2.0.3\n",
            "llama_cpp_python                   0.3.6\n",
            "llvmlite                           0.43.0\n",
            "lm_eval                            0.4.8\n",
            "lm-format-enforcer                 0.10.11\n",
            "locket                             1.0.0\n",
            "logical-unification                0.4.6\n",
            "lxml                               5.3.1\n",
            "Mako                               1.1.3\n",
            "marisa-trie                        1.2.1\n",
            "Markdown                           3.7\n",
            "markdown-it-py                     3.0.0\n",
            "marko                              2.1.2\n",
            "MarkupSafe                         3.0.2\n",
            "marshmallow                        3.26.1\n",
            "matplotlib                         3.10.0\n",
            "matplotlib-inline                  0.1.7\n",
            "matplotlib-venn                    1.1.2\n",
            "mbstrdecoder                       1.1.4\n",
            "mdit-py-plugins                    0.4.2\n",
            "mdurl                              0.1.2\n",
            "miniKanren                         1.0.3\n",
            "missingno                          0.5.2\n",
            "mistral_common                     1.5.4\n",
            "mistune                            3.1.3\n",
            "mizani                             0.13.1\n",
            "mkl                                2025.0.1\n",
            "ml-dtypes                          0.4.1\n",
            "mlxtend                            0.23.4\n",
            "monotonic                          1.6\n",
            "more-itertools                     10.6.0\n",
            "moviepy                            1.0.3\n",
            "mpire                              2.10.2\n",
            "mpmath                             1.3.0\n",
            "msgpack                            1.1.0\n",
            "msgspec                            0.19.0\n",
            "multidict                          6.2.0\n",
            "multipledispatch                   1.0.0\n",
            "multiprocess                       0.70.16\n",
            "multitasking                       0.0.11\n",
            "murmurhash                         1.0.12\n",
            "music21                            9.3.0\n",
            "mypy-extensions                    1.0.0\n",
            "namex                              0.0.8\n",
            "narwhals                           1.32.0\n",
            "natsort                            8.4.0\n",
            "nbclassic                          1.2.0\n",
            "nbclient                           0.10.2\n",
            "nbconvert                          7.16.6\n",
            "nbformat                           5.10.4\n",
            "ndindex                            1.9.2\n",
            "nest-asyncio                       1.6.0\n",
            "networkx                           3.4.2\n",
            "nibabel                            5.3.2\n",
            "ninja                              1.11.1.4\n",
            "nltk                               3.9.1\n",
            "notebook                           6.5.7\n",
            "notebook_shim                      0.2.4\n",
            "numba                              0.60.0\n",
            "numba-cuda                         0.2.0\n",
            "numexpr                            2.10.2\n",
            "numpy                              1.26.4\n",
            "nvidia-cublas-cu12                 12.4.5.8\n",
            "nvidia-cuda-cupti-cu12             12.4.127\n",
            "nvidia-cuda-nvcc-cu12              12.5.82\n",
            "nvidia-cuda-nvrtc-cu12             12.4.127\n",
            "nvidia-cuda-runtime-cu12           12.4.127\n",
            "nvidia-cudnn-cu12                  9.1.0.70\n",
            "nvidia-cufft-cu12                  11.2.1.3\n",
            "nvidia-curand-cu12                 10.3.5.147\n",
            "nvidia-cusolver-cu12               11.6.1.9\n",
            "nvidia-cusparse-cu12               12.3.1.170\n",
            "nvidia-cusparselt-cu12             0.6.2\n",
            "nvidia-ml-py                       12.570.86\n",
            "nvidia-nccl-cu12                   2.21.5\n",
            "nvidia-nvcomp-cu12                 4.2.0.11\n",
            "nvidia-nvjitlink-cu12              12.4.127\n",
            "nvidia-nvtx-cu12                   12.4.127\n",
            "nvtx                               0.2.11\n",
            "nx-cugraph-cu12                    25.2.0\n",
            "oauth2client                       4.1.3\n",
            "oauthlib                           3.2.2\n",
            "openai                             1.69.0\n",
            "opencv-contrib-python              4.11.0.86\n",
            "opencv-python                      4.11.0.86\n",
            "opencv-python-headless             4.11.0.86\n",
            "openpyxl                           3.1.5\n",
            "opentelemetry-api                  1.31.1\n",
            "opentelemetry-sdk                  1.31.1\n",
            "opentelemetry-semantic-conventions 0.52b1\n",
            "opt_einsum                         3.4.0\n",
            "optax                              0.2.4\n",
            "optree                             0.14.1\n",
            "orbax-checkpoint                   0.11.10\n",
            "orjson                             3.10.16\n",
            "osqp                               1.0.1\n",
            "outlines                           0.1.11\n",
            "outlines_core                      0.1.26\n",
            "packaging                          24.2\n",
            "pandas                             2.2.2\n",
            "pandas-datareader                  0.10.0\n",
            "pandas-gbq                         0.28.0\n",
            "pandas-stubs                       2.2.2.240909\n",
            "pandocfilters                      1.5.1\n",
            "panel                              1.6.1\n",
            "param                              2.2.0\n",
            "parso                              0.8.4\n",
            "parsy                              2.1\n",
            "partd                              1.4.2\n",
            "partial-json-parser                0.2.1.1.post5\n",
            "pathlib                            1.0.1\n",
            "pathspec                           0.12.1\n",
            "pathvalidate                       3.2.3\n",
            "patsy                              1.0.1\n",
            "peewee                             3.17.9\n",
            "peft                               0.14.0\n",
            "pexpect                            4.9.0\n",
            "pickleshare                        0.7.5\n",
            "pillow                             10.4.0\n",
            "pip                                24.1.2\n",
            "platformdirs                       4.3.7\n",
            "plotly                             5.24.1\n",
            "plotnine                           0.14.5\n",
            "pluggy                             1.5.0\n",
            "ply                                3.11\n",
            "polars                             1.21.0\n",
            "pooch                              1.8.2\n",
            "portalocker                        3.1.1\n",
            "portpicker                         1.5.2\n",
            "posthog                            3.23.0\n",
            "preshed                            3.0.9\n",
            "prettytable                        3.16.0\n",
            "proglog                            0.1.10\n",
            "progressbar2                       4.5.0\n",
            "prometheus_client                  0.21.1\n",
            "prometheus-fastapi-instrumentator  7.1.0\n",
            "promise                            2.3\n",
            "prompt_toolkit                     3.0.50\n",
            "propcache                          0.3.1\n",
            "prophet                            1.1.6\n",
            "proto-plus                         1.26.1\n",
            "protobuf                           5.29.4\n",
            "psutil                             7.0.0\n",
            "psycopg2                           2.9.10\n",
            "ptyprocess                         0.7.0\n",
            "py-cpuinfo                         9.0.0\n",
            "py4j                               0.10.9.7\n",
            "pyarrow                            18.1.0\n",
            "pyasn1                             0.6.1\n",
            "pyasn1_modules                     0.4.2\n",
            "pybind11                           2.13.6\n",
            "pycairo                            1.27.0\n",
            "pyclipper                          1.3.0.post6\n",
            "pycocotools                        2.0.8\n",
            "pycountry                          24.6.1\n",
            "pycparser                          2.22\n",
            "pydantic                           2.11.0\n",
            "pydantic_core                      2.33.0\n",
            "pydantic-settings                  2.8.1\n",
            "pydantic_yaml                      1.4.0\n",
            "pydata-google-auth                 1.9.1\n",
            "pydot                              3.0.4\n",
            "pydotplus                          2.0.2\n",
            "PyDrive                            1.3.1\n",
            "PyDrive2                           1.21.3\n",
            "pyerfa                             2.0.1.5\n",
            "pygame                             2.6.1\n",
            "pygit2                             1.17.0\n",
            "Pygments                           2.18.0\n",
            "PyGObject                          3.42.0\n",
            "PyJWT                              2.10.1\n",
            "pylatexenc                         2.10\n",
            "pylibcudf-cu12                     25.2.1\n",
            "pylibcugraph-cu12                  25.2.0\n",
            "pylibraft-cu12                     25.2.0\n",
            "pymc                               5.21.1\n",
            "pymystem3                          0.2.0\n",
            "pynndescent                        0.5.13\n",
            "pynvjitlink-cu12                   0.5.2\n",
            "pynvml                             12.0.0\n",
            "pyogrio                            0.10.0\n",
            "Pyomo                              6.8.2\n",
            "PyOpenGL                           3.1.9\n",
            "pyOpenSSL                          24.2.1\n",
            "pyparsing                          3.2.3\n",
            "pypdfium2                          4.30.1\n",
            "pyperclip                          1.9.0\n",
            "pyproj                             3.7.1\n",
            "pyshp                              2.3.1\n",
            "PySocks                            1.7.1\n",
            "pyspark                            3.5.5\n",
            "pytablewriter                      1.2.1\n",
            "pytensor                           2.28.3\n",
            "pytest                             8.3.5\n",
            "python-apt                         0.0.0\n",
            "python-bidi                        0.6.6\n",
            "python-box                         7.3.2\n",
            "python-dateutil                    2.8.2\n",
            "python-docx                        1.1.2\n",
            "python-dotenv                      1.1.0\n",
            "python-louvain                     0.16\n",
            "python-multipart                   0.0.20\n",
            "python-pptx                        1.0.2\n",
            "python-slugify                     8.0.4\n",
            "python-snappy                      0.7.3\n",
            "python-utils                       3.9.1\n",
            "pytz                               2025.2\n",
            "pyviz_comms                        3.0.4\n",
            "PyYAML                             6.0.2\n",
            "pyzmq                              24.0.1\n",
            "raft-dask-cu12                     25.2.0\n",
            "ragas                              0.2.14\n",
            "rapids-dask-dependency             25.2.0\n",
            "ratelim                            0.1.6\n",
            "ray                                2.40.0\n",
            "referencing                        0.36.2\n",
            "regex                              2024.11.6\n",
            "requests                           2.32.3\n",
            "requests-oauthlib                  2.0.0\n",
            "requests-toolbelt                  1.0.0\n",
            "requirements-parser                0.9.0\n",
            "rich                               13.9.4\n",
            "rich-toolkit                       0.14.1\n",
            "rmm-cu12                           25.2.0\n",
            "roman-numerals-py                  3.1.0\n",
            "rouge_score                        0.1.2\n",
            "rpds-py                            0.24.0\n",
            "rpy2                               3.5.17\n",
            "rsa                                4.9\n",
            "rtree                              1.4.0\n",
            "ruamel.yaml                        0.18.10\n",
            "ruamel.yaml.clib                   0.2.12\n",
            "s3transfer                         0.11.4\n",
            "sacrebleu                          2.5.1\n",
            "safetensors                        0.5.3\n",
            "scikit-image                       0.25.2\n",
            "scikit-learn                       1.6.1\n",
            "scipy                              1.14.1\n",
            "scooby                             0.10.0\n",
            "scs                                3.2.7.post2\n",
            "seaborn                            0.13.2\n",
            "SecretStorage                      3.3.1\n",
            "semchunk                           2.2.2\n",
            "Send2Trash                         1.8.3\n",
            "sentence-transformers              3.4.1\n",
            "sentencepiece                      0.2.0\n",
            "sentry-sdk                         2.24.1\n",
            "setproctitle                       1.3.5\n",
            "setuptools                         75.2.0\n",
            "shap                               0.47.1\n",
            "shapely                            2.0.7\n",
            "shellingham                        1.5.4\n",
            "shortuuid                          1.0.13\n",
            "simple-parsing                     0.1.7\n",
            "simplejson                         3.20.1\n",
            "simsimd                            6.2.1\n",
            "six                                1.17.0\n",
            "sklearn-compat                     0.1.3\n",
            "sklearn-pandas                     2.2.0\n",
            "slicer                             0.0.8\n",
            "smart-open                         7.1.0\n",
            "smmap                              5.0.2\n",
            "sniffio                            1.3.1\n",
            "snowballstemmer                    2.2.0\n",
            "sortedcontainers                   2.4.0\n",
            "soundfile                          0.13.1\n",
            "soupsieve                          2.6\n",
            "soxr                               0.5.0.post1\n",
            "spacy                              3.8.4\n",
            "spacy-legacy                       3.0.12\n",
            "spacy-loggers                      1.0.5\n",
            "spanner-graph-notebook             1.1.5\n",
            "Sphinx                             8.2.3\n",
            "sphinxcontrib-applehelp            2.0.0\n",
            "sphinxcontrib-devhelp              2.0.0\n",
            "sphinxcontrib-htmlhelp             2.1.0\n",
            "sphinxcontrib-jsmath               1.0.1\n",
            "sphinxcontrib-qthelp               2.0.0\n",
            "sphinxcontrib-serializinghtml      2.0.0\n",
            "SQLAlchemy                         2.0.40\n",
            "sqlglot                            25.20.2\n",
            "sqlitedict                         2.1.0\n",
            "sqlparse                           0.5.3\n",
            "srsly                              2.5.1\n",
            "sse-starlette                      2.2.1\n",
            "stanio                             0.5.1\n",
            "starlette                          0.46.1\n",
            "starlette-context                  0.3.6\n",
            "statsmodels                        0.14.4\n",
            "stringzilla                        3.12.3\n",
            "sympy                              1.13.1\n",
            "tabledata                          1.3.4\n",
            "tables                             3.10.2\n",
            "tabulate                           0.9.0\n",
            "tbb                                2022.1.0\n",
            "tblib                              3.0.0\n",
            "tcmlib                             1.3.0\n",
            "tcolorpy                           0.1.7\n",
            "tenacity                           9.0.0\n",
            "tensorboard                        2.18.0\n",
            "tensorboard-data-server            0.7.2\n",
            "tensorflow                         2.18.0\n",
            "tensorflow-datasets                4.9.8\n",
            "tensorflow-hub                     0.16.1\n",
            "tensorflow-io-gcs-filesystem       0.37.1\n",
            "tensorflow-metadata                1.16.1\n",
            "tensorflow-probability             0.25.0\n",
            "tensorflow-text                    2.18.1\n",
            "tensorstore                        0.1.72\n",
            "termcolor                          2.5.0\n",
            "terminado                          0.18.1\n",
            "tesserocr                          2.8.0\n",
            "text-unidecode                     1.3\n",
            "textblob                           0.19.0\n",
            "tf_keras                           2.18.0\n",
            "tf-slim                            1.1.0\n",
            "thinc                              8.3.4\n",
            "threadpoolctl                      3.6.0\n",
            "tifffile                           2025.3.13\n",
            "tiktoken                           0.9.0\n",
            "timm                               1.0.15\n",
            "tinycss2                           1.4.0\n",
            "tokenizers                         0.21.1\n",
            "toml                               0.10.2\n",
            "toolz                              0.12.1\n",
            "torch                              2.5.1\n",
            "torchaudio                         2.5.1\n",
            "torchsummary                       1.5.1\n",
            "torchvision                        0.20.1\n",
            "tornado                            6.4.2\n",
            "tqdm                               4.67.1\n",
            "tqdm-multiprocess                  0.0.11\n",
            "traitlets                          5.7.1\n",
            "traittypes                         0.2.1\n",
            "transformers                       4.50.2\n",
            "treelite                           4.4.1\n",
            "treescope                          0.1.9\n",
            "triton                             3.1.0\n",
            "trl                                0.14.0\n",
            "tweepy                             4.15.0\n",
            "typeguard                          4.4.2\n",
            "typepy                             1.3.4\n",
            "typer                              0.12.5\n",
            "types-pytz                         2025.2.0.20250326\n",
            "types-setuptools                   77.0.2.20250328\n",
            "typing_extensions                  4.13.0\n",
            "typing-inspect                     0.9.0\n",
            "typing-inspection                  0.4.0\n",
            "tzdata                             2025.2\n",
            "tzlocal                            5.3.1\n",
            "uc-micro-py                        1.0.3\n",
            "ucx-py-cu12                        0.42.0\n",
            "ucxx-cu12                          0.42.0\n",
            "umap-learn                         0.5.7\n",
            "umf                                0.10.0\n",
            "uritemplate                        4.1.1\n",
            "urllib3                            2.3.0\n",
            "uvicorn                            0.34.0\n",
            "uvloop                             0.21.0\n",
            "vega-datasets                      0.9.0\n",
            "vllm                               0.7.3\n",
            "wadllib                            1.3.6\n",
            "wandb                              0.19.8\n",
            "wasabi                             1.1.3\n",
            "watchfiles                         1.0.4\n",
            "wcwidth                            0.2.13\n",
            "weasel                             0.4.1\n",
            "webcolors                          24.11.1\n",
            "webencodings                       0.5.1\n",
            "websocket-client                   1.8.0\n",
            "websockets                         15.0.1\n",
            "Werkzeug                           3.1.3\n",
            "wheel                              0.45.1\n",
            "widgetsnbextension                 3.6.10\n",
            "word2number                        1.1\n",
            "wordcloud                          1.9.4\n",
            "wrapt                              1.17.2\n",
            "xarray                             2025.1.2\n",
            "xarray-einstats                    0.8.0\n",
            "xdg-base-dirs                      6.0.2\n",
            "xformers                           0.0.28.post3\n",
            "xgboost                            2.1.4\n",
            "xgrammar                           0.1.11\n",
            "xlrd                               2.0.1\n",
            "XlsxWriter                         3.2.2\n",
            "xxhash                             3.5.0\n",
            "xyzservices                        2025.1.0\n",
            "yamllint                           1.37.0\n",
            "yarl                               1.18.3\n",
            "yellowbrick                        1.5\n",
            "yfinance                           0.2.55\n",
            "zict                               3.0.0\n",
            "zipp                               3.21.0\n",
            "zstandard                          0.23.0\n"
          ]
        }
      ],
      "source": [
        "# Run the rest of the notebook by selecting this third cell and choosing \"Runtime->Run cell and below\"\n",
        "!pip cache remove llama_cpp_python\n",
        "!pip install -r ilab/requirements_gpu.txt\n",
        "!pip list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Mlth0Vetawj"
      },
      "source": [
        "Wrap code cell output for ease of reading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "abrPE0P18BWC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "9beebe6b-7636-40fa-d334-4e23f5b30f28"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import HTML, display\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_TTU5vUhXvM",
        "tags": []
      },
      "source": [
        "## Step 1.5 Check Starting Configuration\n",
        "### Check InstructLab Version\n",
        "\n",
        "Check that InstructLab is installed properly and is configured for using a GPU.\n",
        "\n",
        "The first line from 'InstructLab' section will give the InstructLab version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "BqjqIGE06rYE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "6cb341fd-98df-4be9-abb1-ab7b227216b7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "  Device 0: NVIDIA A100-SXM4-40GB, compute capability 8.0, VMM: yes\n",
            "Platform:\n",
            "  sys.version: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n",
            "  sys.platform: linux\n",
            "  os.name: posix\n",
            "  platform.release: 6.1.85+\n",
            "  platform.machine: x86_64\n",
            "  platform.node: 157e5584f97f\n",
            "  platform.python_version: 3.11.11\n",
            "  os-release.ID: ubuntu\n",
            "  os-release.VERSION_ID: 22.04\n",
            "  os-release.PRETTY_NAME: Ubuntu 22.04.4 LTS\n",
            "  memory.total: 83.48 GB\n",
            "  memory.available: 80.94 GB\n",
            "  memory.used: 1.67 GB\n",
            "\n",
            "InstructLab:\n",
            "  instructlab.version: 0.24.3\n",
            "  instructlab-dolomite.version: 0.2.0\n",
            "  instructlab-eval.version: 0.5.1\n",
            "  instructlab-quantize.version: 0.1.0\n",
            "  instructlab-schema.version: 0.4.2\n",
            "  instructlab-sdg.version: 0.7.3\n",
            "  instructlab-training.version: 0.7.0\n",
            "\n",
            "Torch:\n",
            "  torch.version: 2.5.1+cu124\n",
            "  torch.backends.cpu.capability: AVX512\n",
            "  torch.version.cuda: 12.4\n",
            "  torch.version.hip: None\n",
            "  torch.cuda.available: True\n",
            "  torch.backends.cuda.is_built: True\n",
            "  torch.backends.mps.is_built: False\n",
            "  torch.backends.mps.is_available: False\n",
            "  torch.cuda.bf16: True\n",
            "  torch.cuda.current.device: 0\n",
            "  torch.cuda.0.name: NVIDIA A100-SXM4-40GB\n",
            "  torch.cuda.0.free: 39.1 GB\n",
            "  torch.cuda.0.total: 39.6 GB\n",
            "  torch.cuda.0.capability: 8.0 (see https://developer.nvidia.com/cuda-gpus#compute)\n",
            "\n",
            "llama_cpp_python:\n",
            "  llama_cpp_python.version: 0.3.6\n",
            "  llama_cpp_python.supports_gpu_offload: True\n"
          ]
        }
      ],
      "source": [
        "!ilab system info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5EcJoSS6rYD",
        "tags": []
      },
      "source": [
        "<a id=\"IL1_check\"></a>\n",
        "## Perform Imports and Check for a GPU\n",
        "\n",
        "This code cell checks for a GPU in the configuration. This notebook requires a GPU in the configuration to run properly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "kLDdSBiX6rYD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "e58df7bc-7790-446d-aa6d-5a7c023dd416"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch:  2.5 ; cuda:  cu124\n",
            "GPU(s) are Available\n",
            "One GPU of Type:  NVIDIA A100-SXM4-40GB\n",
            "Starting directory: /content/ilab\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from IPython.display import Image, display\n",
        "from datasets import load_dataset\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "import json\n",
        "import subprocess\n",
        "import shutil\n",
        "import ruamel.yaml\n",
        "os.environ['NUMEXPR_MAX_THREADS'] = '64'\n",
        "Norm = \"<p style='font-family:IBM Plex Sans;font-size:20px'>\"\n",
        "\n",
        "notebook_dir='/content/ilab/'\n",
        "os.chdir(notebook_dir)\n",
        "\n",
        "with open('config.json', 'r') as f:\n",
        "    jsonData = json.load(f)\n",
        "\n",
        "## torch and cuda version check\n",
        "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
        "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
        "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
        "\n",
        "if torch.cuda.is_available() is False:\n",
        "    print(\"No GPU in configuration\")\n",
        "else:\n",
        "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "    print(\"GPU(s) are Available\")\n",
        "    gpus=torch.cuda.device_count()\n",
        "    if gpus==1:\n",
        "      gpu_type=torch.cuda.get_device_name(0)\n",
        "      print(\"One GPU of Type: \", gpu_type)\n",
        "    else:\n",
        "      print(\"ERROR: More than 1 GPU in configuration: \",gpus)\n",
        "print(\"Starting directory: \"+ os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv5uMvQ-a-ZF",
        "tags": []
      },
      "source": [
        "<a id=\"IL1_config\"></a>\n",
        "## Step 1.6 Configure InstructLab\n",
        "\n",
        "### Create InstructLab config file\n",
        "The InstructLab configuration is captured in the *config.yaml* file. This step creates the config.yaml file and sets:\n",
        "- **taxomony_path = taxonomy** - the root location of the taxonomy is set to the taxonomy folder in instructlab-latest\n",
        "- **model_path = models/merlinite-7b-lab-Q4_K_M.gguf** - the default model is set to merlinite\n",
        "\n",
        "**Note:** The default directories for InstructLab are the following. If you initialize InstructLab on your own system, it will default to the following:\n",
        "* **Downloaded Models:**  ~/.cache/instructlab/models/ - Contains all downloaded large language models, including the saved output of ones you generate with ilab.\n",
        "* **Synthetic Data:** ~/.local/share/instructlab/datasets/ - Contains data output from the SDG phase, built on modifications to the taxonomy repository.\n",
        "* **Taxonomy:** ~/.local/share/instructlab/taxonomy/ - Contains the skill and knowledge data.\n",
        "* **Training Output:** ~/.local/share/instructlab/checkpoints/ - Contains the output of the training process.\n",
        "* **config.yaml:** ~/.config/instructlab/config.yaml - Contains the config.yaml file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "QX9s4XZx6rYF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "outputId": "892a1eea-c69e-4b20-b94b-8cc02f06ed99"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ilab was already initialized. config.yaml has been deleted. Reinitialized\n",
            "removing taxonomy\n",
            "removing /root/.cache/instructlab\n",
            "removing /root/.config/instructlab\n",
            "removing /root/.local/share/instructlab\n",
            "ilab model is models/granite-7b-lab-Q4_K_M.gguf.\n",
            "#############################################################\n",
            " \n",
            "Running ilab config init\n",
            "\n",
            "----------------------------------------------------\n",
            "         Welcome to the InstructLab CLI\n",
            "  This guide will help you to setup your environment\n",
            "----------------------------------------------------\n",
            "\n",
            "Please provide the following values to initiate the environment [press 'Enter' for default options when prompted]\n",
            "Path to taxonomy repo [/root/.local/share/instructlab/taxonomy]: `taxonomy` seems to not exist or is empty.\n",
            "Should I clone https://github.com/instructlab/taxonomy.git for you? [Y/n]: Cloning https://github.com/instructlab/taxonomy.git...\n",
            "Path to your model [/root/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf]: \n",
            "Generating config file:\n",
            "    /root/.config/instructlab/config.yaml\n",
            "\n",
            "INFO 2025-04-01 07:42:16,502 instructlab.config.init:262: Detecting hardware...\n",
            "\u001b[32mPlease choose a system profile.\n",
            "Profiles set hardware-specific defaults for all commands and sections of the configuration.\u001b[0m\n",
            "\u001b[37m\u001b[44mFirst, please select the hardware vendor your system falls into\u001b[0m\n",
            "[0] NO SYSTEM PROFILE\n",
            "[1] INTEL\n",
            "[2] AMD\n",
            "[3] NVIDIA\n",
            "[4] APPLE\n",
            "Enter the number of your choice [0]: No profile selected - ilab will use generic code defaults - these may not be optimized for your system.\n",
            "\n",
            "--------------------------------------------\n",
            "    Initialization completed successfully!\n",
            "  You're ready to start using `ilab`. Enjoy!\n",
            "--------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#Remove Colab Sample directory\n",
        "if os.path.exists(\"sample_data\"):\n",
        "    print(\"removing sample_data\")\n",
        "    shutil.rmtree(\"sample_data\")\n",
        "    os.chdir(\"ilab\")\n",
        "\n",
        "#Initialize ilab\n",
        "base_dir=\"/root/\"\n",
        "##Choose the base model as granite or mixtral\n",
        "model_dir=\"models\"\n",
        "model_name=\"granite-7b-lab-Q4_K_M.gguf\"\n",
        "model_path = os.path.join(model_dir, model_name)\n",
        "\n",
        "taxonomy_path='taxonomy'\n",
        "\n",
        "## Define the file name\n",
        "file_name = \"config.yaml\"\n",
        "if os.path.exists(file_name):\n",
        "    os.remove(file_name)\n",
        "    print(f\"ilab was already initialized. {file_name} has been deleted. Reinitialized\")\n",
        "else:\n",
        "    print(f\"ilab was not initialized yet. {file_name} does not exist.\")\n",
        "\n",
        "##Remove old data\n",
        "if os.path.exists(\"taxonomy\"):\n",
        "    print(\"removing taxonomy\")\n",
        "    shutil.rmtree(\"taxonomy\")\n",
        "if os.path.exists(base_dir+\".cache/instructlab\"):\n",
        "    print(\"removing \" + base_dir+\".cache/instructlab\")\n",
        "    shutil.rmtree(base_dir+\".cache/instructlab\")\n",
        "if os.path.exists(base_dir+\".config/instructlab\"):\n",
        "    print(\"removing \" + base_dir+\".config/instructlab\")\n",
        "    shutil.rmtree(base_dir+\".config/instructlab\")\n",
        "if os.path.exists(base_dir+\".local/share/instructlab\"):\n",
        "    print(\"removing \" + base_dir+\".local/share/instructlab\")\n",
        "    shutil.rmtree(base_dir+\".local/share/instructlab\")\n",
        "\n",
        "print(f\"ilab model is {model_path}.\")\n",
        "print('#############################################################')\n",
        "print(' ')\n",
        "\n",
        "command = f\"\"\"\n",
        "ilab config init<<EOF\n",
        "{taxonomy_path}\n",
        "Y\n",
        "{model_path}\n",
        "0\n",
        "EOF\n",
        "\"\"\"\n",
        "\n",
        "## Using the ! operator to run the command\n",
        "!echo \"Running ilab config init\"\n",
        "!{command}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8rbeRKE6rYF"
      },
      "source": [
        "### Display the config.yaml file\n",
        "We examine the base configuration for identifying parameters for changing in the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "ZflN-eeu6rYF",
        "scrolled": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7ce324a8-1633-4d1c-ce83-88969f02a2df"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Chat configuration section.\n",
            "chat:\n",
            "  # Predefined setting or environment that influences the behavior and responses of\n",
            "  # the chat assistant. Each context is associated with a specific prompt that\n",
            "  # guides the assistant on how to respond to user inputs. Available contexts:\n",
            "  # default, cli_helper.\n",
            "  # Default: default\n",
            "  context: default\n",
            "  # Directory where chat logs are stored.\n",
            "  # Default: /root/.local/share/instructlab/chatlogs\n",
            "  logs_dir: /root/.local/share/instructlab/chatlogs\n",
            "  # The maximum number of tokens that can be generated in the chat completion. Be\n",
            "  # aware that larger values use more memory.\n",
            "  # Default: None\n",
            "  max_tokens:\n",
            "  # Model to be used for chatting with.\n",
            "  # Default: /root/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf\n",
            "  model: models/granite-7b-lab-Q4_K_M.gguf\n",
            "  # Filepath of a dialog session file.\n",
            "  # Default: None\n",
            "  session:\n",
            "  # Controls the randomness of the model's responses. Lower values make the output\n",
            "  # more deterministic, while higher values produce more random results.\n",
            "  # Default: 1.0\n",
            "  temperature: 1.0\n",
            "  # Enable vim keybindings for chat.\n",
            "  # Default: False\n",
            "  vi_mode: false\n",
            "  # Renders vertical overflow if enabled, displays ellipses otherwise.\n",
            "  # Default: True\n",
            "  visible_overflow: true\n",
            "# Evaluate configuration section.\n",
            "evaluate:\n",
            "  # Base taxonomy branch\n",
            "  # Default: None\n",
            "  base_branch:\n",
            "  # Base model to compare with 'model' for mt_bench_branch and mmlu_branch.\n",
            "  # Default: instructlab/granite-7b-lab\n",
            "  base_model: instructlab/granite-7b-lab\n",
            "  # Taxonomy branch containing custom skills/knowledge that should be used for\n",
            "  # evaluation runs.\n",
            "  # Default: None\n",
            "  branch:\n",
            "  # Settings to run DK-Bench against a file of user created questions, reference\n",
            "  # answers, and responses. If responses are not provided they are generated from a\n",
            "  # model\n",
            "  dk_bench:\n",
            "    # File with questions and reference answers used for evaluation during DK-Bench.\n",
            "    # The file must be valid a '.jsonl' file with the fields 'user_input' and\n",
            "    # 'reference' in each entry\n",
            "    # Default: None\n",
            "    input_questions:\n",
            "    # Judge model for DK-Bench.\n",
            "    # Default: gpt-4o\n",
            "    judge_model: gpt-4o\n",
            "    # Directory where DK-Bench evaluation results are stored.\n",
            "    # Default: /root/.local/share/instructlab/internal/eval_data/dk_bench\n",
            "    output_dir: /root/.local/share/instructlab/internal/eval_data/dk_bench\n",
            "    # Comma-separated list of file formats for results of the DK-Bench evaluation. Ex:\n",
            "    # 'csv,jsonl'. Valid options in the list are csv, jsonl, and xlsx. If this option\n",
            "    # is not provided the results are written as a .jsonl file\n",
            "    # Default: jsonl\n",
            "    output_file_formats: jsonl\n",
            "  # Number of GPUs to use for running evaluation.\n",
            "  # Default: None\n",
            "  gpus:\n",
            "  # MMLU benchmarking settings\n",
            "  mmlu:\n",
            "    # Batch size for evaluation. Valid values are a positive integer or 'auto' to\n",
            "    # select the largest batch size that will fit in memory.\n",
            "    # Default: auto\n",
            "    batch_size: auto\n",
            "    # Number of question-answer pairs provided in the context preceding the question\n",
            "    # used for evaluation.\n",
            "    # Default: 5\n",
            "    few_shots: 5\n",
            "  # Settings to run MMLU against a branch of taxonomy containing custom\n",
            "  # skills/knowledge used for training.\n",
            "  mmlu_branch:\n",
            "    # Directory where custom MMLU tasks are stored.\n",
            "    # Default: /root/.local/share/instructlab/datasets\n",
            "    tasks_dir: /root/.local/share/instructlab/datasets\n",
            "  # Model to be evaluated\n",
            "  # Default: None\n",
            "  model: models/granite-7b-lab-Q4_K_M.gguf\n",
            "  # Multi-turn benchmarking settings for skills.\n",
            "  mt_bench:\n",
            "    # Judge model for MT-Bench.\n",
            "    # Default: prometheus-eval/prometheus-8x7b-v2.0\n",
            "    judge_model: prometheus-eval/prometheus-8x7b-v2.0\n",
            "    # Number of workers to use for evaluation with mt_bench or mt_bench_branch. Must\n",
            "    # be a positive integer or 'auto'.\n",
            "    # Default: auto\n",
            "    max_workers: auto\n",
            "    # Directory where MT-Bench evaluation results are stored.\n",
            "    # Default: /root/.local/share/instructlab/internal/eval_data/mt_bench\n",
            "    output_dir: /root/.local/share/instructlab/internal/eval_data/mt_bench\n",
            "  # Settings to run MT-Bench against a branch of taxonomy containing custom\n",
            "  # skills/knowledge used for training\n",
            "  mt_bench_branch:\n",
            "    # Judge model for MT-Bench-Branch.\n",
            "    # Default: prometheus-eval/prometheus-8x7b-v2.0\n",
            "    judge_model: prometheus-eval/prometheus-8x7b-v2.0\n",
            "    # Directory where MT-Bench-Branch evaluation results are stored.\n",
            "    # Default: /root/.local/share/instructlab/internal/eval_data/mt_bench_branch\n",
            "    output_dir: /root/.local/share/instructlab/internal/eval_data/mt_bench_branch\n",
            "    # Path to where base taxonomy is stored.\n",
            "    # Default: /root/.local/share/instructlab/taxonomy\n",
            "    taxonomy_path: taxonomy\n",
            "  # System prompt for model getting responses during DK-Bench.\n",
            "  # Default: None\n",
            "  system_prompt:\n",
            "  # Temperature for model getting responses during DK-Bench. Temperature controls\n",
            "  # the randomness of the model's responses. Lower values make the output more\n",
            "  # deterministic, while higher values produce more random results.\n",
            "  # Default: 0.0\n",
            "  temperature: 0.0\n",
            "# General configuration section.\n",
            "general:\n",
            "  # Debug level for logging.\n",
            "  # Default: 0\n",
            "  debug_level: 0\n",
            "  # Log format. https://docs.python.org/3/library/logging.html#logrecord-attributes\n",
            "  # Default: %(levelname)s %(asctime)s %(name)s:%(lineno)d: %(message)s\n",
            "  log_format: '%(levelname)s %(asctime)s %(name)s:%(lineno)d: %(message)s'\n",
            "  # Log level for logging.\n",
            "  # Default: INFO\n",
            "  log_level: INFO\n",
            "  # Use legacy IBM Granite chat template (default uses 3.0 Instruct template)\n",
            "  # Default: False\n",
            "  use_legacy_tmpl: false\n",
            "# Generate configuration section.\n",
            "generate:\n",
            "  # Number of Batches to send for generation on each core.\n",
            "  # Default: 8\n",
            "  batch_size: 8\n",
            "  # Maximum number of words per chunk.\n",
            "  # Default: 1000\n",
            "  chunk_word_count: 1000\n",
            "  # The maximum amount of tokens for the model to generate during knowledge\n",
            "  # generation. A lower number yields less data but a faster SDG run. It is\n",
            "  # reccomended to use this on consumer hardware\n",
            "  # Default: 4096\n",
            "  max_num_tokens: 4096\n",
            "  # Teacher model that will be used to synthetically generate training data.\n",
            "  # Default: /root/.cache/instructlab/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\n",
            "  model: /root/.cache/instructlab/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\n",
            "  # Number of CPU cores to use for generation.\n",
            "  # Default: 10\n",
            "  num_cpus: 10\n",
            "  # Number of instructions to use\n",
            "  # Default: -1\n",
            "  # Deprecated: see 'sdg_scale_factor' instead\n",
            "  num_instructions: -1\n",
            "  # Directory where generated datasets are stored.\n",
            "  # Default: /root/.local/share/instructlab/datasets\n",
            "  output_dir: /root/.local/share/instructlab/datasets\n",
            "  # Data generation pipeline to use. Available: 'simple', 'full', or a valid path to\n",
            "  # a directory of pipeline workflow YAML files. Note that 'full' requires a larger\n",
            "  # teacher model, Mixtral-8x7b.\n",
            "  # Default: full\n",
            "  pipeline: full\n",
            "  # The total number of instructions to be generated.\n",
            "  # Default: 30\n",
            "  sdg_scale_factor: 30\n",
            "  # Branch of taxonomy used to calculate diff against.\n",
            "  # Default: origin/main\n",
            "  taxonomy_base: origin/main\n",
            "  # Directory where taxonomy is stored and accessed from.\n",
            "  # Default: /root/.local/share/instructlab/taxonomy\n",
            "  taxonomy_path: taxonomy\n",
            "  # Teacher configuration\n",
            "  teacher:\n",
            "    # Serving backend to use to host the model.\n",
            "    # Default: None\n",
            "    # Examples:\n",
            "    #   - vllm\n",
            "    #   - llama-cpp\n",
            "    backend:\n",
            "    # Chat template to supply to the model. Possible values: 'auto'(default),\n",
            "    # 'tokenizer', a path to a jinja2 file.\n",
            "    # Default: None\n",
            "    # Examples:\n",
            "    #   - auto\n",
            "    #   - tokenizer\n",
            "    #   - A filesystem path expressing the location of a custom template\n",
            "    chat_template:\n",
            "    # llama-cpp serving settings.\n",
            "    llama_cpp:\n",
            "      # Number of model layers to offload to GPU. -1 means all layers.\n",
            "      # Default: -1\n",
            "      gpu_layers: -1\n",
            "      # Large Language Model Family\n",
            "      # Default: ''\n",
            "      # Examples:\n",
            "      #   - granite\n",
            "      #   - mixtral\n",
            "      llm_family: ''\n",
            "      # Maximum number of tokens that can be processed by the model.\n",
            "      # Default: 4096\n",
            "      max_ctx_size: 4096\n",
            "    # Directory where model to be served is stored.\n",
            "    # Default: /root/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf\n",
            "    model_path: /root/.cache/instructlab/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\n",
            "    # Server configuration including host and port.\n",
            "    # Default: host='127.0.0.1' port=8000 backend_type='' current_max_ctx_size=4096\n",
            "    server:\n",
            "      # Backend Instance Type\n",
            "      # Default: ''\n",
            "      # Examples:\n",
            "      #   - llama-cpp\n",
            "      #   - vllm\n",
            "      backend_type: ''\n",
            "      # Maximum number of tokens that can be processed by the currently served model.\n",
            "      # Default: 4096\n",
            "      current_max_ctx_size: 4096\n",
            "      # Host to serve on.\n",
            "      # Default: 127.0.0.1\n",
            "      host: 127.0.0.1\n",
            "      # Port to serve on.\n",
            "      # Default: 8000\n",
            "      port: 8000\n",
            "    # vLLM serving settings.\n",
            "    vllm:\n",
            "      # Number of GPUs to use.\n",
            "      # Default: None\n",
            "      gpus:\n",
            "      # Large Language Model Family\n",
            "      # Default: ''\n",
            "      # Examples:\n",
            "      #   - granite\n",
            "      #   - mixtral\n",
            "      llm_family: ''\n",
            "      # Maximum number of attempts to start the vLLM server.\n",
            "      # Default: 120\n",
            "      max_startup_attempts: 120\n",
            "      # vLLM specific arguments. All settings can be passed as a list of strings, see:\n",
            "      # https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html\n",
            "      # Default: []\n",
            "      # Examples:\n",
            "      #   - ['--dtype', 'auto']\n",
            "      #   - ['--lora-alpha', '32']\n",
            "      vllm_args: []\n",
            "# Metadata pertaining to the specifics of the system which the Configuration is\n",
            "# meant to be applied to.\n",
            "metadata:\n",
            "  # Manufacturer, Family, and SKU of the system CPU, ex: Apple M3 Max\n",
            "  # Default: None\n",
            "  cpu_info:\n",
            "  # Amount of GPUs on the system, ex: 8\n",
            "  # Default: None\n",
            "  gpu_count:\n",
            "  # Family of the system GPU, ex: H100\n",
            "  # Default: None\n",
            "  gpu_family:\n",
            "  # Manufacturer of the system GPU, ex: Nvidia\n",
            "  # Default: None\n",
            "  gpu_manufacturer:\n",
            "  # Specific SKU related information about the given GPU, ex: PCIe, NVL\n",
            "  # Default: None\n",
            "  gpu_sku:\n",
            "# RAG configuration section.\n",
            "rag:\n",
            "  # RAG convert configuration section.\n",
            "  convert:\n",
            "    # Directory where converted documents are stored.\n",
            "    # Default: /root/.local/share/instructlab/converted_documents\n",
            "    output_dir: /root/.local/share/instructlab/converted_documents\n",
            "    # Branch of taxonomy used to calculate diff against.\n",
            "    # Default: origin/main\n",
            "    taxonomy_base: origin/main\n",
            "    # Directory where taxonomy is stored and accessed from.\n",
            "    # Default: /root/.local/share/instructlab/taxonomy\n",
            "    taxonomy_path: /root/.local/share/instructlab/taxonomy\n",
            "  # Document store configuration for RAG.\n",
            "  document_store:\n",
            "    # Document store collection name.\n",
            "    # Default: ilab\n",
            "    collection_name: ilab\n",
            "    # Document store service URI.\n",
            "    # Default: /root/.local/share/instructlab/embeddings.db\n",
            "    uri: /root/.local/share/instructlab/embeddings.db\n",
            "  # Embedding model configuration for RAG\n",
            "  embedding_model:\n",
            "    # Embedding model to use for RAG.\n",
            "    # Default: /root/.cache/instructlab/models/ibm-granite/granite-embedding-125m-english\n",
            "    embedding_model_path: /root/.cache/instructlab/models/ibm-granite/granite-embedding-125m-english\n",
            "  # Flag for enabling RAG functionality.\n",
            "  # Default: False\n",
            "  enabled: false\n",
            "  # Retrieval configuration parameters for RAG\n",
            "  retriever:\n",
            "    # The maximum number of documents to retrieve.\n",
            "    # Default: 3\n",
            "    top_k: 3\n",
            "# Serve configuration section.\n",
            "serve:\n",
            "  # Serving backend to use to host the model.\n",
            "  # Default: None\n",
            "  # Examples:\n",
            "  #   - vllm\n",
            "  #   - llama-cpp\n",
            "  backend:\n",
            "  # Chat template to supply to the model. Possible values: 'auto'(default),\n",
            "  # 'tokenizer', a path to a jinja2 file.\n",
            "  # Default: None\n",
            "  # Examples:\n",
            "  #   - auto\n",
            "  #   - tokenizer\n",
            "  #   - A filesystem path expressing the location of a custom template\n",
            "  chat_template:\n",
            "  # llama-cpp serving settings.\n",
            "  llama_cpp:\n",
            "    # Number of model layers to offload to GPU. -1 means all layers.\n",
            "    # Default: -1\n",
            "    gpu_layers: -1\n",
            "    # Large Language Model Family\n",
            "    # Default: ''\n",
            "    # Examples:\n",
            "    #   - granite\n",
            "    #   - mixtral\n",
            "    llm_family: ''\n",
            "    # Maximum number of tokens that can be processed by the model.\n",
            "    # Default: 4096\n",
            "    max_ctx_size: 4096\n",
            "  # Directory where model to be served is stored.\n",
            "  # Default: /root/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf\n",
            "  model_path: models/granite-7b-lab-Q4_K_M.gguf\n",
            "  # Server configuration including host and port.\n",
            "  # Default: host='127.0.0.1' port=8000 backend_type='' current_max_ctx_size=4096\n",
            "  server:\n",
            "    # Backend Instance Type\n",
            "    # Default: ''\n",
            "    # Examples:\n",
            "    #   - llama-cpp\n",
            "    #   - vllm\n",
            "    backend_type: ''\n",
            "    # Maximum number of tokens that can be processed by the currently served model.\n",
            "    # Default: 4096\n",
            "    current_max_ctx_size: 4096\n",
            "    # Host to serve on.\n",
            "    # Default: 127.0.0.1\n",
            "    host: 127.0.0.1\n",
            "    # Port to serve on.\n",
            "    # Default: 8000\n",
            "    port: 8000\n",
            "  # vLLM serving settings.\n",
            "  vllm:\n",
            "    # Number of GPUs to use.\n",
            "    # Default: None\n",
            "    gpus:\n",
            "    # Large Language Model Family\n",
            "    # Default: ''\n",
            "    # Examples:\n",
            "    #   - granite\n",
            "    #   - mixtral\n",
            "    llm_family: ''\n",
            "    # Maximum number of attempts to start the vLLM server.\n",
            "    # Default: 120\n",
            "    max_startup_attempts: 120\n",
            "    # vLLM specific arguments. All settings can be passed as a list of strings, see:\n",
            "    # https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html\n",
            "    # Default: []\n",
            "    # Examples:\n",
            "    #   - ['--dtype', 'auto']\n",
            "    #   - ['--lora-alpha', '32']\n",
            "    vllm_args: []\n",
            "# Train configuration section.\n",
            "train:\n",
            "  # Additional arguments to pass to the training script. These arguments are passed\n",
            "  # as key-value pairs to the training script.\n",
            "  # Default: {}\n",
            "  additional_args: {}\n",
            "  # Save a checkpoint at the end of each epoch.\n",
            "  # Default: True\n",
            "  checkpoint_at_epoch: true\n",
            "  # Directory where periodic training checkpoints are stored.\n",
            "  # Default: /root/.local/share/instructlab/checkpoints\n",
            "  ckpt_output_dir: /root/.local/share/instructlab/checkpoints\n",
            "  # Directory where the processed training data is stored (post\n",
            "  # filtering/tokenization/masking).\n",
            "  # Default: /root/.local/share/instructlab/internal\n",
            "  data_output_dir: /root/.local/share/instructlab/internal\n",
            "  # For the training library (primary training method), this specifies the path to\n",
            "  # the dataset file. For legacy training (MacOS/Linux), this specifies the path to\n",
            "  # the directory.\n",
            "  # Default: /root/.local/share/instructlab/datasets\n",
            "  data_path: /root/.local/share/instructlab/datasets\n",
            "  # Allow CPU offload for deepspeed optimizer.\n",
            "  # Default: False\n",
            "  deepspeed_cpu_offload_optimizer: false\n",
            "  # PyTorch device to use. Use 'cpu' for 'simple' and 'full' training on Linux. Use\n",
            "  # 'mps' for 'full' training on MacOS Metal Performance Shader. Use 'cuda' for\n",
            "  # Nvidia CUDA / AMD ROCm GPUs. Use 'hpu' for Intel Gaudi GPUs.\n",
            "  # Default: cpu\n",
            "  # Examples:\n",
            "  #   - cpu\n",
            "  #   - mps\n",
            "  #   - cuda\n",
            "  #   - hpu\n",
            "  device: cpu\n",
            "  # Whether or not we should disable the use of flash-attention during training.\n",
            "  # This is useful when using older GPUs.\n",
            "  # Default: False\n",
            "  disable_flash_attn: false\n",
            "  # Pick a distributed training backend framework for GPU accelerated full fine-\n",
            "  # tuning.\n",
            "  # Default: fsdp\n",
            "  distributed_backend: fsdp\n",
            "  # The number of samples in a batch that the model should see before its parameters\n",
            "  # are updated.\n",
            "  # Default: 64\n",
            "  effective_batch_size: 64\n",
            "  # Allow CPU offload for FSDP optimizer.\n",
            "  # Default: False\n",
            "  fsdp_cpu_offload_optimizer: false\n",
            "  # Boolean to indicate if the model being trained is a padding-free transformer\n",
            "  # model such as Granite.\n",
            "  # Default: False\n",
            "  is_padding_free: false\n",
            "  # The data type for quantization in LoRA training. Valid options are 'None' and\n",
            "  # 'nf4'.\n",
            "  # Default: nf4\n",
            "  # Examples:\n",
            "  #   - nf4\n",
            "  lora_quantize_dtype: nf4\n",
            "  # Rank of low rank matrices to be used during training.\n",
            "  # Default: 0\n",
            "  lora_rank: 0\n",
            "  # Maximum tokens per gpu for each batch that will be handled in a single step. If\n",
            "  # running into out-of-memory errors, this value can be lowered but not below the\n",
            "  # `max_seq_len`.\n",
            "  # Default: 5000\n",
            "  max_batch_len: 5000\n",
            "  # Maximum sequence length to be included in the training set. Samples exceeding\n",
            "  # this length will be dropped.\n",
            "  # Default: 4096\n",
            "  max_seq_len: 4096\n",
            "  # Directory where the model to be trained is stored.\n",
            "  # Default: instructlab/granite-7b-lab\n",
            "  model_path: instructlab/granite-7b-lab\n",
            "  # Number of GPUs to use for training. This value is not supported in legacy\n",
            "  # training or MacOS.\n",
            "  # Default: 1\n",
            "  nproc_per_node: 1\n",
            "  # Number of epochs to run training for.\n",
            "  # Default: 10\n",
            "  num_epochs: 10\n",
            "  # Base directory for organization of end-to-end intermediate outputs.\n",
            "  # Default: /root/.local/share/instructlab/phased\n",
            "  phased_base_dir: /root/.local/share/instructlab/phased\n",
            "  # Judge model path for phased MT-Bench evaluation.\n",
            "  # Default: /root/.cache/instructlab/models/prometheus-eval/prometheus-8x7b-v2.0\n",
            "  phased_mt_bench_judge: /root/.cache/instructlab/models/prometheus-eval/prometheus-8x7b-v2.0\n",
            "  # Phased phase1 effective batch size.\n",
            "  # Default: 128\n",
            "  phased_phase1_effective_batch_size: 128\n",
            "  # Learning rate for phase1 knowledge training.\n",
            "  # Default: 2e-05\n",
            "  phased_phase1_learning_rate: 2e-05\n",
            "  # Number of epochs to run training for during phase1 (experimentally optimal\n",
            "  # number is 7).\n",
            "  # Default: 7\n",
            "  phased_phase1_num_epochs: 7\n",
            "  # Number of samples the model should see before saving a checkpoint during phase1.\n",
            "  # Disabled when set to 0.\n",
            "  # Default: 0\n",
            "  phased_phase1_samples_per_save: 0\n",
            "  # Phased phase2 effective batch size.\n",
            "  # Default: 3840\n",
            "  phased_phase2_effective_batch_size: 3840\n",
            "  # Learning rate for phase2 skills training.\n",
            "  # Default: 6e-06\n",
            "  phased_phase2_learning_rate: 6e-06\n",
            "  # Number of epochs to run training for during phase2.\n",
            "  # Default: 10\n",
            "  phased_phase2_num_epochs: 10\n",
            "  # Number of samples the model should see before saving a checkpoint during phase2.\n",
            "  # Disabled when set to 0.\n",
            "  # Default: 0\n",
            "  phased_phase2_samples_per_save: 0\n",
            "  # Training pipeline to use. Simple is for systems with limited resources, full is\n",
            "  # for more capable consumer systems (64 GB of RAM), and accelerated is for systems\n",
            "  # with a dedicated GPU.\n",
            "  # Default: full\n",
            "  # Examples:\n",
            "  #   - simple\n",
            "  #   - full\n",
            "  #   - accelerated\n",
            "  pipeline: full\n",
            "  # Number of samples the model should see before saving a checkpoint.\n",
            "  # Default: 250000\n",
            "  save_samples: 250000\n",
            "  # Optional path to a yaml file that tracks the progress of multiphase training.\n",
            "  # Default: None\n",
            "  training_journal:\n",
            "# Configuration file structure version.\n",
            "# Default: 1.0.0\n",
            "version: 1.0.0\n"
          ]
        }
      ],
      "source": [
        "##to copy config.yaml to local directory\n",
        "!cp /root/.config/instructlab/config.yaml .\n",
        "!cat config.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAL743VA6rYH"
      },
      "source": [
        "### Customize LLM Models and copy to notebook for use\n",
        "\n",
        "This cell changes the models to use for the generate stage. The mistral model as the teacher model in the generate step and as the student model to be trained.\n",
        "\n",
        "If you want to customize other models for generation or the training phase, you would specify the models in this step.\n",
        "\n",
        "This step specifies that the models to be used will be from this notebook's models directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "QILAQZYY6rYH",
        "scrolled": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "eca180bf-ff52-451a-b2d7-b9633d172241"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated config.yaml successfully.\n",
            "\n",
            "# Chat configuration section.\n",
            "chat:\n",
            "  # Predefined setting or environment that influences the behavior and responses of\n",
            "  # the chat assistant. Each context is associated with a specific prompt that\n",
            "  # guides the assistant on how to respond to user inputs. Available contexts:\n",
            "  # default, cli_helper.\n",
            "  # Default: default\n",
            "  context: default\n",
            "  # Directory where chat logs are stored.\n",
            "  # Default: /root/.local/share/instructlab/chatlogs\n",
            "  logs_dir: /root/.local/share/instructlab/chatlogs\n",
            "  # The maximum number of tokens that can be generated in the chat completion. Be\n",
            "  # aware that larger values use more memory.\n",
            "  # Default: None\n",
            "  max_tokens:\n",
            "  # Model to be used for chatting with.\n",
            "  # Default: /root/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf\n",
            "  model: models/granite-7b-lab-Q4_K_M.gguf\n",
            "  # Filepath of a dialog session file.\n",
            "  # Default: None\n",
            "  session:\n",
            "  # Controls the randomness of the model's responses. Lower values make the output\n",
            "  # more deterministic, while higher values produce more random results.\n",
            "  # Default: 1.0\n",
            "  temperature: 1.0\n",
            "  # Enable vim keybindings for chat.\n",
            "  # Default: False\n",
            "  vi_mode: false\n",
            "  # Renders vertical overflow if enabled, displays ellipses otherwise.\n",
            "  # Default: True\n",
            "  visible_overflow: true\n",
            "# Evaluate configuration section.\n",
            "evaluate:\n",
            "  # Base taxonomy branch\n",
            "  # Default: None\n",
            "  base_branch:\n",
            "  # Base model to compare with 'model' for mt_bench_branch and mmlu_branch.\n",
            "  # Default: instructlab/granite-7b-lab\n",
            "  base_model: instructlab/granite-7b-lab\n",
            "  # Taxonomy branch containing custom skills/knowledge that should be used for\n",
            "  # evaluation runs.\n",
            "  # Default: None\n",
            "  branch:\n",
            "  # Settings to run DK-Bench against a file of user created questions, reference\n",
            "  # answers, and responses. If responses are not provided they are generated from a\n",
            "  # model\n",
            "  dk_bench:\n",
            "    # File with questions and reference answers used for evaluation during DK-Bench.\n",
            "    # The file must be valid a '.jsonl' file with the fields 'user_input' and\n",
            "    # 'reference' in each entry\n",
            "    # Default: None\n",
            "    input_questions:\n",
            "    # Judge model for DK-Bench.\n",
            "    # Default: gpt-4o\n",
            "    judge_model: gpt-4o\n",
            "    # Directory where DK-Bench evaluation results are stored.\n",
            "    # Default: /root/.local/share/instructlab/internal/eval_data/dk_bench\n",
            "    output_dir: /root/.local/share/instructlab/internal/eval_data/dk_bench\n",
            "    # Comma-separated list of file formats for results of the DK-Bench evaluation. Ex:\n",
            "    # 'csv,jsonl'. Valid options in the list are csv, jsonl, and xlsx. If this option\n",
            "    # is not provided the results are written as a .jsonl file\n",
            "    # Default: jsonl\n",
            "    output_file_formats: jsonl\n",
            "  # Number of GPUs to use for running evaluation.\n",
            "  # Default: None\n",
            "  gpus: 1\n",
            "  # MMLU benchmarking settings\n",
            "  mmlu:\n",
            "    # Batch size for evaluation. Valid values are a positive integer or 'auto' to\n",
            "    # select the largest batch size that will fit in memory.\n",
            "    # Default: auto\n",
            "    batch_size: auto\n",
            "    # Number of question-answer pairs provided in the context preceding the question\n",
            "    # used for evaluation.\n",
            "    # Default: 5\n",
            "    few_shots: 5\n",
            "  # Settings to run MMLU against a branch of taxonomy containing custom\n",
            "  # skills/knowledge used for training.\n",
            "  mmlu_branch:\n",
            "    # Directory where custom MMLU tasks are stored.\n",
            "    # Default: /root/.local/share/instructlab/datasets\n",
            "    tasks_dir: /root/.local/share/instructlab/datasets\n",
            "  # Model to be evaluated\n",
            "  # Default: None\n",
            "  model: models/granite-7b-lab-Q4_K_M.gguf\n",
            "  # Multi-turn benchmarking settings for skills.\n",
            "  mt_bench:\n",
            "    # Judge model for MT-Bench.\n",
            "    # Default: prometheus-eval/prometheus-8x7b-v2.0\n",
            "    judge_model: prometheus-eval/prometheus-8x7b-v2.0\n",
            "    # Number of workers to use for evaluation with mt_bench or mt_bench_branch. Must\n",
            "    # be a positive integer or 'auto'.\n",
            "    # Default: auto\n",
            "    max_workers: auto\n",
            "    # Directory where MT-Bench evaluation results are stored.\n",
            "    # Default: /root/.local/share/instructlab/internal/eval_data/mt_bench\n",
            "    output_dir: /root/.local/share/instructlab/internal/eval_data/mt_bench\n",
            "  # Settings to run MT-Bench against a branch of taxonomy containing custom\n",
            "  # skills/knowledge used for training\n",
            "  mt_bench_branch:\n",
            "    # Judge model for MT-Bench-Branch.\n",
            "    # Default: prometheus-eval/prometheus-8x7b-v2.0\n",
            "    judge_model: prometheus-eval/prometheus-8x7b-v2.0\n",
            "    # Directory where MT-Bench-Branch evaluation results are stored.\n",
            "    # Default: /root/.local/share/instructlab/internal/eval_data/mt_bench_branch\n",
            "    output_dir: /root/.local/share/instructlab/internal/eval_data/mt_bench_branch\n",
            "    # Path to where base taxonomy is stored.\n",
            "    # Default: /root/.local/share/instructlab/taxonomy\n",
            "    taxonomy_path: taxonomy\n",
            "  # System prompt for model getting responses during DK-Bench.\n",
            "  # Default: None\n",
            "  system_prompt:\n",
            "  # Temperature for model getting responses during DK-Bench. Temperature controls\n",
            "  # the randomness of the model's responses. Lower values make the output more\n",
            "  # deterministic, while higher values produce more random results.\n",
            "  # Default: 0.0\n",
            "  temperature: 0.0\n",
            "# General configuration section.\n",
            "general:\n",
            "  # Debug level for logging.\n",
            "  # Default: 0\n",
            "  debug_level: 0\n",
            "  # Log format. https://docs.python.org/3/library/logging.html#logrecord-attributes\n",
            "  # Default: %(levelname)s %(asctime)s %(name)s:%(lineno)d: %(message)s\n",
            "  log_format: '%(levelname)s %(asctime)s %(name)s:%(lineno)d: %(message)s'\n",
            "  # Log level for logging.\n",
            "  # Default: INFO\n",
            "  log_level: INFO\n",
            "  # Use legacy IBM Granite chat template (default uses 3.0 Instruct template)\n",
            "  # Default: False\n",
            "  use_legacy_tmpl: false\n",
            "# Generate configuration section.\n",
            "generate:\n",
            "  # Number of Batches to send for generation on each core.\n",
            "  # Default: 8\n",
            "  batch_size: 8\n",
            "  # Maximum number of words per chunk.\n",
            "  # Default: 1000\n",
            "  chunk_word_count: 1000\n",
            "  # The maximum amount of tokens for the model to generate during knowledge\n",
            "  # generation. A lower number yields less data but a faster SDG run. It is\n",
            "  # reccomended to use this on consumer hardware\n",
            "  # Default: 4096\n",
            "  max_num_tokens: 4096\n",
            "  # Teacher model that will be used to synthetically generate training data.\n",
            "  # Default: /root/.cache/instructlab/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\n",
            "  model: models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\n",
            "  # Number of CPU cores to use for generation.\n",
            "  # Default: 10\n",
            "  num_cpus: 10\n",
            "  # Number of instructions to use\n",
            "  # Default: -1\n",
            "  # Deprecated: see 'sdg_scale_factor' instead\n",
            "  num_instructions: -1\n",
            "  # Directory where generated datasets are stored.\n",
            "  # Default: /root/.local/share/instructlab/datasets\n",
            "  output_dir: /root/.local/share/instructlab/datasets\n",
            "  # Data generation pipeline to use. Available: 'simple', 'full', or a valid path to\n",
            "  # a directory of pipeline workflow YAML files. Note that 'full' requires a larger\n",
            "  # teacher model, Mixtral-8x7b.\n",
            "  # Default: full\n",
            "  pipeline: full\n",
            "  # The total number of instructions to be generated.\n",
            "  # Default: 30\n",
            "  sdg_scale_factor: 30\n",
            "  # Branch of taxonomy used to calculate diff against.\n",
            "  # Default: origin/main\n",
            "  taxonomy_base: origin/main\n",
            "  # Directory where taxonomy is stored and accessed from.\n",
            "  # Default: /root/.local/share/instructlab/taxonomy\n",
            "  taxonomy_path: taxonomy\n",
            "  # Teacher configuration\n",
            "  teacher:\n",
            "    # Serving backend to use to host the model.\n",
            "    # Default: None\n",
            "    # Examples:\n",
            "    #   - vllm\n",
            "    #   - llama-cpp\n",
            "    backend:\n",
            "    # Chat template to supply to the model. Possible values: 'auto'(default),\n",
            "    # 'tokenizer', a path to a jinja2 file.\n",
            "    # Default: None\n",
            "    # Examples:\n",
            "    #   - auto\n",
            "    #   - tokenizer\n",
            "    #   - A filesystem path expressing the location of a custom template\n",
            "    chat_template:\n",
            "    # llama-cpp serving settings.\n",
            "    llama_cpp:\n",
            "      # Number of model layers to offload to GPU. -1 means all layers.\n",
            "      # Default: -1\n",
            "      gpu_layers: -1\n",
            "      # Large Language Model Family\n",
            "      # Default: ''\n",
            "      # Examples:\n",
            "      #   - granite\n",
            "      #   - mixtral\n",
            "      llm_family: ''\n",
            "      # Maximum number of tokens that can be processed by the model.\n",
            "      # Default: 4096\n",
            "      max_ctx_size: 4096\n",
            "    # Directory where model to be served is stored.\n",
            "    # Default: /root/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf\n",
            "    model_path: models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\n",
            "    # Server configuration including host and port.\n",
            "    # Default: host='127.0.0.1' port=8000 backend_type='' current_max_ctx_size=4096\n",
            "    server:\n",
            "      # Backend Instance Type\n",
            "      # Default: ''\n",
            "      # Examples:\n",
            "      #   - llama-cpp\n",
            "      #   - vllm\n",
            "      backend_type: ''\n",
            "      # Maximum number of tokens that can be processed by the currently served model.\n",
            "      # Default: 4096\n",
            "      current_max_ctx_size: 4096\n",
            "      # Host to serve on.\n",
            "      # Default: 127.0.0.1\n",
            "      host: 127.0.0.1\n",
            "      # Port to serve on.\n",
            "      # Default: 8000\n",
            "      port: 8000\n",
            "    # vLLM serving settings.\n",
            "    vllm:\n",
            "      # Number of GPUs to use.\n",
            "      # Default: None\n",
            "      gpus: 1\n",
            "      # Large Language Model Family\n",
            "      # Default: ''\n",
            "      # Examples:\n",
            "      #   - granite\n",
            "      #   - mixtral\n",
            "      llm_family: ''\n",
            "      # Maximum number of attempts to start the vLLM server.\n",
            "      # Default: 120\n",
            "      max_startup_attempts: 120\n",
            "      # vLLM specific arguments. All settings can be passed as a list of strings, see:\n",
            "      # https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html\n",
            "      # Default: []\n",
            "      # Examples:\n",
            "      #   - ['--dtype', 'auto']\n",
            "      #   - ['--lora-alpha', '32']\n",
            "      vllm_args: []\n",
            "# Metadata pertaining to the specifics of the system which the Configuration is\n",
            "# meant to be applied to.\n",
            "metadata:\n",
            "  # Manufacturer, Family, and SKU of the system CPU, ex: Apple M3 Max\n",
            "  # Default: None\n",
            "  cpu_info:\n",
            "  # Amount of GPUs on the system, ex: 8\n",
            "  # Default: None\n",
            "  gpu_count: 1\n",
            "  # Family of the system GPU, ex: H100\n",
            "  # Default: None\n",
            "  gpu_family: A100-SXM4-40GB\n",
            "  # Manufacturer of the system GPU, ex: Nvidia\n",
            "  # Default: None\n",
            "  gpu_manufacturer: Nvidia\n",
            "  # Specific SKU related information about the given GPU, ex: PCIe, NVL\n",
            "  # Default: None\n",
            "  gpu_sku:\n",
            "# RAG configuration section.\n",
            "rag:\n",
            "  # RAG convert configuration section.\n",
            "  convert:\n",
            "    # Directory where converted documents are stored.\n",
            "    # Default: /root/.local/share/instructlab/converted_documents\n",
            "    output_dir: /root/.local/share/instructlab/converted_documents\n",
            "    # Branch of taxonomy used to calculate diff against.\n",
            "    # Default: origin/main\n",
            "    taxonomy_base: origin/main\n",
            "    # Directory where taxonomy is stored and accessed from.\n",
            "    # Default: /root/.local/share/instructlab/taxonomy\n",
            "    taxonomy_path: /root/.local/share/instructlab/taxonomy\n",
            "  # Document store configuration for RAG.\n",
            "  document_store:\n",
            "    # Document store collection name.\n",
            "    # Default: ilab\n",
            "    collection_name: ilab\n",
            "    # Document store service URI.\n",
            "    # Default: /root/.local/share/instructlab/embeddings.db\n",
            "    uri: /root/.local/share/instructlab/embeddings.db\n",
            "  # Embedding model configuration for RAG\n",
            "  embedding_model:\n",
            "    # Embedding model to use for RAG.\n",
            "    # Default: /root/.cache/instructlab/models/ibm-granite/granite-embedding-125m-english\n",
            "    embedding_model_path: /root/.cache/instructlab/models/ibm-granite/granite-embedding-125m-english\n",
            "  # Flag for enabling RAG functionality.\n",
            "  # Default: False\n",
            "  enabled: false\n",
            "  # Retrieval configuration parameters for RAG\n",
            "  retriever:\n",
            "    # The maximum number of documents to retrieve.\n",
            "    # Default: 3\n",
            "    top_k: 3\n",
            "# Serve configuration section.\n",
            "serve:\n",
            "  # Serving backend to use to host the model.\n",
            "  # Default: None\n",
            "  # Examples:\n",
            "  #   - vllm\n",
            "  #   - llama-cpp\n",
            "  backend:\n",
            "  # Chat template to supply to the model. Possible values: 'auto'(default),\n",
            "  # 'tokenizer', a path to a jinja2 file.\n",
            "  # Default: None\n",
            "  # Examples:\n",
            "  #   - auto\n",
            "  #   - tokenizer\n",
            "  #   - A filesystem path expressing the location of a custom template\n",
            "  chat_template:\n",
            "  # llama-cpp serving settings.\n",
            "  llama_cpp:\n",
            "    # Number of model layers to offload to GPU. -1 means all layers.\n",
            "    # Default: -1\n",
            "    gpu_layers: -1\n",
            "    # Large Language Model Family\n",
            "    # Default: ''\n",
            "    # Examples:\n",
            "    #   - granite\n",
            "    #   - mixtral\n",
            "    llm_family: ''\n",
            "    # Maximum number of tokens that can be processed by the model.\n",
            "    # Default: 4096\n",
            "    max_ctx_size: 4096\n",
            "  # Directory where model to be served is stored.\n",
            "  # Default: /root/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf\n",
            "  model_path: models/granite-7b-lab-Q4_K_M.gguf\n",
            "  # Server configuration including host and port.\n",
            "  # Default: host='127.0.0.1' port=8000 backend_type='' current_max_ctx_size=4096\n",
            "  server:\n",
            "    # Backend Instance Type\n",
            "    # Default: ''\n",
            "    # Examples:\n",
            "    #   - llama-cpp\n",
            "    #   - vllm\n",
            "    backend_type: ''\n",
            "    # Maximum number of tokens that can be processed by the currently served model.\n",
            "    # Default: 4096\n",
            "    current_max_ctx_size: 4096\n",
            "    # Host to serve on.\n",
            "    # Default: 127.0.0.1\n",
            "    host: 127.0.0.1\n",
            "    # Port to serve on.\n",
            "    # Default: 8000\n",
            "    port: 8000\n",
            "  # vLLM serving settings.\n",
            "  vllm:\n",
            "    # Number of GPUs to use.\n",
            "    # Default: None\n",
            "    gpus: 1\n",
            "    # Large Language Model Family\n",
            "    # Default: ''\n",
            "    # Examples:\n",
            "    #   - granite\n",
            "    #   - mixtral\n",
            "    llm_family: ''\n",
            "    # Maximum number of attempts to start the vLLM server.\n",
            "    # Default: 120\n",
            "    max_startup_attempts: 120\n",
            "    # vLLM specific arguments. All settings can be passed as a list of strings, see:\n",
            "    # https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html\n",
            "    # Default: []\n",
            "    # Examples:\n",
            "    #   - ['--dtype', 'auto']\n",
            "    #   - ['--lora-alpha', '32']\n",
            "    vllm_args: []\n",
            "# Train configuration section.\n",
            "train:\n",
            "  # Additional arguments to pass to the training script. These arguments are passed\n",
            "  # as key-value pairs to the training script.\n",
            "  # Default: {}\n",
            "  additional_args: {}\n",
            "  # Save a checkpoint at the end of each epoch.\n",
            "  # Default: True\n",
            "  checkpoint_at_epoch: true\n",
            "  # Directory where periodic training checkpoints are stored.\n",
            "  # Default: /root/.local/share/instructlab/checkpoints\n",
            "  ckpt_output_dir: /root/.local/share/instructlab/checkpoints\n",
            "  # Directory where the processed training data is stored (post\n",
            "  # filtering/tokenization/masking).\n",
            "  # Default: /root/.local/share/instructlab/internal\n",
            "  data_output_dir: /root/.local/share/instructlab/internal\n",
            "  # For the training library (primary training method), this specifies the path to\n",
            "  # the dataset file. For legacy training (MacOS/Linux), this specifies the path to\n",
            "  # the directory.\n",
            "  # Default: /root/.local/share/instructlab/datasets\n",
            "  data_path: /root/.local/share/instructlab/datasets\n",
            "  # Allow CPU offload for deepspeed optimizer.\n",
            "  # Default: False\n",
            "  deepspeed_cpu_offload_optimizer: false\n",
            "  # PyTorch device to use. Use 'cpu' for 'simple' and 'full' training on Linux. Use\n",
            "  # 'mps' for 'full' training on MacOS Metal Performance Shader. Use 'cuda' for\n",
            "  # Nvidia CUDA / AMD ROCm GPUs. Use 'hpu' for Intel Gaudi GPUs.\n",
            "  # Default: cpu\n",
            "  # Examples:\n",
            "  #   - cpu\n",
            "  #   - mps\n",
            "  #   - cuda\n",
            "  #   - hpu\n",
            "  device: cuda\n",
            "  # Whether or not we should disable the use of flash-attention during training.\n",
            "  # This is useful when using older GPUs.\n",
            "  # Default: False\n",
            "  disable_flash_attn: false\n",
            "  # Pick a distributed training backend framework for GPU accelerated full fine-\n",
            "  # tuning.\n",
            "  # Default: fsdp\n",
            "  distributed_backend: fsdp\n",
            "  # The number of samples in a batch that the model should see before its parameters\n",
            "  # are updated.\n",
            "  # Default: 64\n",
            "  effective_batch_size: 64\n",
            "  # Allow CPU offload for FSDP optimizer.\n",
            "  # Default: False\n",
            "  fsdp_cpu_offload_optimizer: false\n",
            "  # Boolean to indicate if the model being trained is a padding-free transformer\n",
            "  # model such as Granite.\n",
            "  # Default: False\n",
            "  is_padding_free: false\n",
            "  # The data type for quantization in LoRA training. Valid options are 'None' and\n",
            "  # 'nf4'.\n",
            "  # Default: nf4\n",
            "  # Examples:\n",
            "  #   - nf4\n",
            "  lora_quantize_dtype: nf4\n",
            "  # Rank of low rank matrices to be used during training.\n",
            "  # Default: 0\n",
            "  lora_rank: 0\n",
            "  # Maximum tokens per gpu for each batch that will be handled in a single step. If\n",
            "  # running into out-of-memory errors, this value can be lowered but not below the\n",
            "  # `max_seq_len`.\n",
            "  # Default: 5000\n",
            "  max_batch_len: 5000\n",
            "  # Maximum sequence length to be included in the training set. Samples exceeding\n",
            "  # this length will be dropped.\n",
            "  # Default: 4096\n",
            "  max_seq_len: 4096\n",
            "  # Directory where the model to be trained is stored.\n",
            "  # Default: instructlab/granite-7b-lab\n",
            "  model_path: instructlab/granite-7b-lab\n",
            "  # Number of GPUs to use for training. This value is not supported in legacy\n",
            "  # training or MacOS.\n",
            "  # Default: 1\n",
            "  nproc_per_node: 1\n",
            "  # Number of epochs to run training for.\n",
            "  # Default: 10\n",
            "  num_epochs: 10\n",
            "  # Base directory for organization of end-to-end intermediate outputs.\n",
            "  # Default: /root/.local/share/instructlab/phased\n",
            "  phased_base_dir: /root/.local/share/instructlab/phased\n",
            "  # Judge model path for phased MT-Bench evaluation.\n",
            "  # Default: /root/.cache/instructlab/models/prometheus-eval/prometheus-8x7b-v2.0\n",
            "  phased_mt_bench_judge: /root/.cache/instructlab/models/prometheus-eval/prometheus-8x7b-v2.0\n",
            "  # Phased phase1 effective batch size.\n",
            "  # Default: 128\n",
            "  phased_phase1_effective_batch_size: 128\n",
            "  # Learning rate for phase1 knowledge training.\n",
            "  # Default: 2e-05\n",
            "  phased_phase1_learning_rate: 2e-05\n",
            "  # Number of epochs to run training for during phase1 (experimentally optimal\n",
            "  # number is 7).\n",
            "  # Default: 7\n",
            "  phased_phase1_num_epochs: 7\n",
            "  # Number of samples the model should see before saving a checkpoint during phase1.\n",
            "  # Disabled when set to 0.\n",
            "  # Default: 0\n",
            "  phased_phase1_samples_per_save: 0\n",
            "  # Phased phase2 effective batch size.\n",
            "  # Default: 3840\n",
            "  phased_phase2_effective_batch_size: 3840\n",
            "  # Learning rate for phase2 skills training.\n",
            "  # Default: 6e-06\n",
            "  phased_phase2_learning_rate: 6e-06\n",
            "  # Number of epochs to run training for during phase2.\n",
            "  # Default: 10\n",
            "  phased_phase2_num_epochs: 10\n",
            "  # Number of samples the model should see before saving a checkpoint during phase2.\n",
            "  # Disabled when set to 0.\n",
            "  # Default: 0\n",
            "  phased_phase2_samples_per_save: 0\n",
            "  # Training pipeline to use. Simple is for systems with limited resources, full is\n",
            "  # for more capable consumer systems (64 GB of RAM), and accelerated is for systems\n",
            "  # with a dedicated GPU.\n",
            "  # Default: full\n",
            "  # Examples:\n",
            "  #   - simple\n",
            "  #   - full\n",
            "  #   - accelerated\n",
            "  pipeline: full\n",
            "  # Number of samples the model should see before saving a checkpoint.\n",
            "  # Default: 250000\n",
            "  save_samples: 250000\n",
            "  # Optional path to a yaml file that tracks the progress of multiphase training.\n",
            "  # Default: None\n",
            "  training_journal:\n",
            "# Configuration file structure version.\n",
            "# Default: 1.0.0\n",
            "version: 1.0.0\n"
          ]
        }
      ],
      "source": [
        "##Use ruamel.yaml to load the yaml file to preserve comments\n",
        "yaml = ruamel.yaml.YAML()\n",
        "with open('config.yaml', 'r') as file:\n",
        "    config = yaml.load(file)\n",
        "\n",
        "##Upate to use the same models and just change the directory\n",
        "teacher_model_path = \"models/mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n",
        "base_model_path = \"models/instructlab/granite-7b-lab\"\n",
        "##judge_model_path = \"models/prometheus-eval/prometheus-8x7b-v2.0\"\n",
        "\n",
        "##config['evaluate']['mt_bench']['judge_model'] = judge_model_path\n",
        "##config['evaluate']['mt_bench_branch']['judge_model'] = judge_model_path\n",
        "config['generate']['model'] = teacher_model_path\n",
        "config['generate']['teacher']['model_path']= teacher_model_path\n",
        "##config['train']['phased_mt_bench_judge']=judge_model_path\n",
        "\n",
        "#Update GPU information\n",
        "config['evaluate']['gpus']=gpus\n",
        "config['generate']['teacher']['vllm']['gpus']=gpus\n",
        "config['serve']['vllm']['gpus']=gpus\n",
        "config['train']['nproc_per_node']=gpus\n",
        "config['metadata']['gpu_count']=gpus\n",
        "if gpus==1:\n",
        "  config['train']['device']=\"cuda\"\n",
        "  if gpu_type[:6]==\"NVIDIA\":\n",
        "    config['metadata']['gpu_manufacturer']=\"Nvidia\"\n",
        "    config['metadata']['gpu_family']=gpu_type[7:]\n",
        "\n",
        "## Save the updated config.yaml file\n",
        "yaml.default_flow_style=False\n",
        "with open('config.yaml', 'w') as file:\n",
        "    yaml.dump(config, file)\n",
        "\n",
        "##copy the config file to the .config/instructlab/ where it is used by InstructLab\n",
        "!cp config.yaml {base_dir}.config/instructlab/\n",
        "\n",
        "print(\"Updated config.yaml successfully.\\n\")\n",
        "!cat config.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dZgO-FZ6rYH"
      },
      "source": [
        "<a id=\"IL1_down\"></a>\n",
        "## Step 1.7 Download Models\n",
        "The models that will be used in the InstructLab processing are downloaded in this step. Additional steps can be added if other models are used in processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI-25zVd6rYH"
      },
      "source": [
        "The merlinite model will be used as the teacher model for the simple pipeline in the **Training with InstructLab** section.\n",
        "\n",
        "The mistral-7b-instruct-v0.2.Q4_K_M model will be used as the teacher model for the full pipeline in that section.\n",
        "\n",
        "The granite07b-lab.gguf model is a quantized version of the granite-7b-lab model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "rCa2va8r6rYH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "32977559-ea2f-477d-9b21-7a632299a34a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 2025-04-01 07:42:18,655 instructlab.model.download:77: Downloading model from Hugging Face:\n",
            "    Model: instructlab/granite-7b-lab-GGUF@main\n",
            "    Destination: models\n",
            "INFO 2025-04-01 07:42:18,782 instructlab.model.download:288: \n",
            "ᕦ(òᴗóˇ)ᕤ instructlab/granite-7b-lab-GGUF model download completed successfully! ᕦ(òᴗóˇ)ᕤ\n",
            "\n",
            "INFO 2025-04-01 07:42:18,782 instructlab.model.download:77: Downloading model from Hugging Face:\n",
            "    Model: instructlab/merlinite-7b-lab-GGUF@main\n",
            "    Destination: models\n",
            "INFO 2025-04-01 07:42:18,909 instructlab.model.download:288: \n",
            "ᕦ(òᴗóˇ)ᕤ instructlab/merlinite-7b-lab-GGUF model download completed successfully! ᕦ(òᴗóˇ)ᕤ\n",
            "\n",
            "INFO 2025-04-01 07:42:18,910 instructlab.model.download:77: Downloading model from Hugging Face:\n",
            "    Model: TheBloke/Mistral-7B-Instruct-v0.2-GGUF@main\n",
            "    Destination: models\n",
            "INFO 2025-04-01 07:42:18,998 instructlab.model.download:288: \n",
            "ᕦ(òᴗóˇ)ᕤ TheBloke/Mistral-7B-Instruct-v0.2-GGUF model download completed successfully! ᕦ(òᴗóˇ)ᕤ\n",
            "\n",
            "INFO 2025-04-01 07:42:18,998 instructlab.model.download:77: Downloading model from Hugging Face:\n",
            "    Model: ibm-granite/granite-embedding-125m-english@main\n",
            "    Destination: models\n",
            "INFO 2025-04-01 07:42:19,238 instructlab.model.download:288: \n",
            "ᕦ(òᴗóˇ)ᕤ ibm-granite/granite-embedding-125m-english model download completed successfully! ᕦ(òᴗóˇ)ᕤ\n",
            "\n",
            "INFO 2025-04-01 07:42:19,238 instructlab.model.download:302: Available models (`ilab model list`):\n",
            "+--------------------------------------------+---------------------+----------+-----------------------------------------------------------+\n",
            "| Model Name                                 | Last Modified       | Size     | Absolute path                                             |\n",
            "+--------------------------------------------+---------------------+----------+-----------------------------------------------------------+\n",
            "| mistral-7b-instruct-v0.2.Q4_K_M.gguf       | 2025-04-01 07:33:43 | 4.1 GB   | /content/ilab/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf |\n",
            "| granite-7b-lab-Q4_K_M.gguf                 | 2025-04-01 07:33:19 | 3.8 GB   | /content/ilab/models/granite-7b-lab-Q4_K_M.gguf           |\n",
            "| instructlab/granite-7b-lab                 | 2025-04-01 07:34:31 | 12.6 GB  | /content/ilab/models/instructlab                          |\n",
            "| ibm-granite/granite-embedding-125m-english | 2025-04-01 07:33:48 | 479.2 MB | /content/ilab/models/ibm-granite                          |\n",
            "| merlinite-7b-lab-Q4_K_M.gguf               | 2025-04-01 07:33:31 | 4.1 GB   | /content/ilab/models/merlinite-7b-lab-Q4_K_M.gguf         |\n",
            "+--------------------------------------------+---------------------+----------+-----------------------------------------------------------+\n",
            "INFO 2025-04-01 07:42:22,229 instructlab.model.download:77: Downloading model from Hugging Face:\n",
            "    Model: instructlab/granite-7b-lab@main\n",
            "    Destination: models\n",
            "INFO 2025-04-01 07:42:22,399 instructlab.model.download:288: \n",
            "ᕦ(òᴗóˇ)ᕤ instructlab/granite-7b-lab model download completed successfully! ᕦ(òᴗóˇ)ᕤ\n",
            "\n",
            "INFO 2025-04-01 07:42:22,400 instructlab.model.download:302: Available models (`ilab model list`):\n",
            "+--------------------------------------------+---------------------+----------+-----------------------------------------------------------+\n",
            "| Model Name                                 | Last Modified       | Size     | Absolute path                                             |\n",
            "+--------------------------------------------+---------------------+----------+-----------------------------------------------------------+\n",
            "| mistral-7b-instruct-v0.2.Q4_K_M.gguf       | 2025-04-01 07:33:43 | 4.1 GB   | /content/ilab/models/mistral-7b-instruct-v0.2.Q4_K_M.gguf |\n",
            "| granite-7b-lab-Q4_K_M.gguf                 | 2025-04-01 07:33:19 | 3.8 GB   | /content/ilab/models/granite-7b-lab-Q4_K_M.gguf           |\n",
            "| instructlab/granite-7b-lab                 | 2025-04-01 07:34:31 | 12.6 GB  | /content/ilab/models/instructlab                          |\n",
            "| ibm-granite/granite-embedding-125m-english | 2025-04-01 07:33:48 | 479.2 MB | /content/ilab/models/ibm-granite                          |\n",
            "| merlinite-7b-lab-Q4_K_M.gguf               | 2025-04-01 07:33:31 | 4.1 GB   | /content/ilab/models/merlinite-7b-lab-Q4_K_M.gguf         |\n",
            "+--------------------------------------------+---------------------+----------+-----------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "models_dir=\"models\"\n",
        "!ilab model download --hf-token {hf_token} --model-dir {models_dir}\n",
        "\n",
        "# Download the granite 7b safe tensors model\n",
        "!ilab model download --repository instructlab/granite-7b-lab --hf-token {hf_token} --model-dir {models_dir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDUmbbHLvxgg",
        "tags": []
      },
      "source": [
        "<a id=\"IL2_0\"></a>\n",
        "# Section 2. Generating Synthetic Data\n",
        "\n",
        "\n",
        "This section demonstrates training with InstructLab. This section is part of a sequential notebook. Before running this section of the notebook, please ensure that you have run the Configuring InstructLab section of this notebook.\n",
        "\n",
        "In this section, we will demonstrate:\n",
        "- Creating a question and answer data file\n",
        "- Generating synthetic data for training\n",
        "- Training the LLM with the generated data\n",
        "\n",
        "The steps in this section are as follows:\n",
        "* Step 2.1 Specify the Data for this Run\n",
        "* Step 2.2 Create the Taxonomy Data Repository\n",
        "* Step 2.3 Generate Synthetic Data\n",
        "* Step 2.4 Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnOLAXRxvxgh",
        "tags": []
      },
      "source": [
        "<a id=\"IL2_data\"></a>\n",
        "## Step 2.1 Specify the Data for this Run\n",
        "\n",
        "We've provided question-and-answer files for these datasets: \"2024 Oscar Awards Ceremony\", \"Quantum Roadmap and Patterns\" and \"Artificial Intelligence Agents\". Feel free to choose one of these datasets, or select your own custom dataset in the cell below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltRTPBTVvxgh",
        "tags": []
      },
      "source": [
        "### Optionally, Create your own data set for InstructLab\n",
        "\n",
        "You can optionally provide your own InstructLab QNA file for processing in this step.\n",
        "\n",
        "Follow these steps to add your own dataset:\n",
        "1. Create your own qna.yaml file following the directions on the InstructLab taxonomy [readme](https://github.com/instructlab/taxonomy).\n",
        "1. Create a questions.txt file with related sample questions to use on inferencing.\n",
        "1. Add your qna.yaml and sample questions.txt files to the /content/ilab/data/your_content_1 folder or the /content/ilab/data/your_content_2 folder by dragging and dropping them in the desired folder.\n",
        "1. Double click on the /content/ilab/config.json file to edit and specify the qna_location where your data resides within the Dewey Decimal classification system. Close and save the config.json file.\n",
        "1. You can now specify to run with your own data by selecting **Your Content 1** or **Your Content 2** in the next code cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "DbnDz_4j5GNi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "0e9f1575-b8e1-4c7f-99f4-8eafddcab27c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Select the QNA dataset to add:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'cybersecurity'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After choosing your dataset, please select and run the following cell\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nSelect the QNA dataset to add:\")\n",
        "display(data_set)\n",
        "print(\"After choosing your dataset, please select and run the following cell\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "olkDUaKfvxgh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "21498648-6b05-4854-a6bf-e195f4e8c9a1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 2.2 Choose the Dataset for this Run\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'value'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-b70b9f4e11fe>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Step 2.2 Choose the Dataset for this Run\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mdata_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'2024 Oscars'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0muse_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"oscars\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mdata_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'Quantum'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0muse_case\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"quantum\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'value'"
          ]
        }
      ],
      "source": [
        "print(\"Step 2.2 Choose the Dataset for this Run\")\n",
        "if data_set.value=='cybersecurity':\n",
        "    use_case=\"cybersecurity\"\n",
        "else:\n",
        "    use_case=\"undefined\"\n",
        "\n",
        "if use_case==\"undefined\":\n",
        "    print(\"ERROR: Undefined data set: \" + data_set.value + \" data\")\n",
        "else:\n",
        "    qna_file=\"data/\" + use_case + \"/qna.yaml\"\n",
        "    qna_location=jsonData[\"use_cases\"][use_case][\"qna_location\"]\n",
        "\n",
        "    print(\"Using \" + data_set.value + \" data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQBSnoObvxgh"
      },
      "source": [
        "<a id=\"IL2_taxonomy\"></a>\n",
        "## Step 2.2 Create the Taxonomy Data Repository\n",
        "Delete the prior repository, clone the empty taxonomy repository and place the QNA file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2s_UhRCvxgh"
      },
      "outputs": [],
      "source": [
        "#Delete the prior repository and clone the empty taxonomy repository\n",
        "print(\"Delete the prior repository and clone the empty taxonomy repository\")\n",
        "shell_command1 = f\"rm -rf taxonomy\"\n",
        "taxonomy_repo=jsonData[\"taxonomy_repo\"]\n",
        "shell_command2 = f\"git clone {taxonomy_repo}\"\n",
        "!{shell_command1}\n",
        "!{shell_command2}\n",
        "\n",
        "#show the QNA file\n",
        "print(\"Show the QNA file\")\n",
        "print_lines=40\n",
        "with open(qna_file, 'r') as input_file:\n",
        "    for line_number, line in enumerate(input_file):\n",
        "        if line_number > print_lines:  # line_number starts at 0.\n",
        "            break\n",
        "        print(line, end=\"\")\n",
        "\n",
        "# Place the QNA file in the proper taxonomy directory\n",
        "print(\"Place QNA file in taxononmy as: /taxonomy/\"+qna_location+\"/qna.yaml\")\n",
        "shell_command1 = f\"mkdir -p ./taxonomy/{qna_location}\"\n",
        "shell_command2 = f\"cp ./{qna_file} ./taxonomy/{qna_location}/qna.yaml\"\n",
        "!{shell_command1}\n",
        "!{shell_command2}\n",
        "\n",
        "print(\"Verify the taxonomy\")\n",
        "!ilab taxonomy diff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFKW-EIVvxgi",
        "tags": []
      },
      "source": [
        "<a id=\"IL2_generate\"></a>\n",
        "## Step 2.3. Set data generation parameters\n",
        "\n",
        "#### Select pipeline\n",
        "\n",
        "InstructLab has three primary pipelines that can be used: simple, full and acellerated:\n",
        "- The **simple pipeline** runs fast and can be used for initial model and data testing.\n",
        "- The **full pipeline** runs all of the InstrctLab steps and takes more time but produces a better tuned model.\n",
        "\n",
        "**Note:** If you are running with a new or modifed dataset, you may want to use the **Simple pipeline** for the first run to verify the configuration\n",
        "\n",
        "#### Sepect number of samples to generate\n",
        "\n",
        "Data generation takes 19 minutes for generating 15 synthetic data samples. You may wish to generate a small number on your first run to verify the QNA dataset format.\n",
        "\n",
        "To produce **sufficient synthetic data** to focus training on the new material, **about 30 synthetic questions and answer pairs need to be generated** for each question and answer pair provided. This will require a proportionally longer time to generate, but will provide better training.\n",
        "\n",
        "Before following these instructions, ensure the existing model you are adding skills or knowledge to is still running. Alternatively, ilab data generate can start a server for you if you provide a fully qualified model path via --model.\n",
        "\n",
        "To generate a synthetic dataset based on your newly added knowledge or skill set in taxonomy repository, run the following command:\n",
        "\n",
        "    ilab data generate\n",
        "\n",
        "#### **Simple Pipeline**\n",
        "\n",
        "The Simple Pipeline works solely with Merlinite 7b Lab as the teacher model. The Simple Pipeline is called without GPU acceleration as follows:\n",
        "\n",
        "    ilab data generate --pipeline simple\n",
        "\n",
        "#### **Full Pipeline**\n",
        "\n",
        "The Full Pipeline runs the full processing with a GPU. Currently, the Full Pipeline only supports the Mixtral and Mistral Instruct Family models as the teacher model.  This is due to only supporting specific model prompt templates.\n",
        "\n",
        "Using a non-default model such as Mixtral-8x7B-Instruct-v0.1) to generate data with the Full Pipeline:\n",
        "\n",
        "    ilab data generate --model ~/.cache/instructlab/models/mistralai/mixtral-8x7b-instruct-v0.1 --pipeline full --gpus 4\n",
        "\n",
        "**Note** Synthetic Data Generation can take from 2 minutes to 1+ hours to complete, depending on your computing resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rNYESyEvxgi",
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(\"Select Pipeline to use\")\n",
        "display(sdg_pipe)\n",
        "display(instr)\n",
        "print(\"After making your selections for data generation, please select and run the following cell\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-so0dOw4vxgi",
        "tags": []
      },
      "source": [
        "### 2.4 Run data generation\n",
        "Data generation with a GPU can take 2 minutes or more to generate 15 synthetic data samples. It takes proportionately longer to generate more samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wn6gODEcvxgi",
        "tags": []
      },
      "outputs": [],
      "source": [
        "gen_directory = \"data/\"+ use_case+\"/ilab_generated/\"\n",
        "if instr.value == \"Default (>450)\":\n",
        "        sdg_factor=\"\"\n",
        "elif instr.value == '>15':\n",
        "    sdg_factor=\"--sdg-scale-factor 1\"\n",
        "elif instr.value == '>50':\n",
        "    sdg_factor=\"--sdg-scale-factor 3\"\n",
        "elif instr.value == '>200':\n",
        "    sdg_factor=\"--sdg-scale-factor 13\"\n",
        "elif instr.value == '>500':\n",
        "    sdg_factor=\"--sdg-scale-factor 33\"\n",
        "else:\n",
        "    sdg_factor=\"--sdg-scale-factor 67\"\n",
        "# 'Fast (Simple)', 'Full with CPU'\n",
        "if sdg_pipe.value == 'Simple':\n",
        "    pipeline = 'simple'\n",
        "    model = '--model models/instructlab/granite-7b-lab'\n",
        "    gpus = '--gpus 1'\n",
        "elif sdg_pipe.value == 'Full with GPU':\n",
        "    pipeline = 'full'\n",
        "    model = ''\n",
        "#   model = '--model models/instructlab/granite-7b-lab'\n",
        "    gpus = '--gpus 1'\n",
        "else:\n",
        "    print(\"ERROR: Undefined pipeline\")\n",
        "\n",
        "il_data_path= '/root/.local/share/instructlab/datasets/'\n",
        "#Remove old data so there is only one test_merlinite and train_merlinite after generation\n",
        "print(\"Remove old datasets\")\n",
        "!rm -rf {il_data_path}*\n",
        "#shell_command = f\"ilab --verbose data generate {model} --num-cpus 10 {gpus} {sdg_factor} --taxonomy-path taxonomy --pipeline {pipeline} --max-num-tokens 512\"\n",
        "shell_command = f\"ilab data generate {model} --num-cpus 10 {gpus} {sdg_factor} --taxonomy-path taxonomy --pipeline {pipeline} --max-num-tokens 512\"\n",
        "\n",
        "print(\"Generating data\")\n",
        "print(\"Running: !\"+shell_command)\n",
        "!{shell_command}\n",
        "\n",
        "#Rename results to  test_gen.jsonl and train_gen.jsonl and move to local data directory\n",
        "if not os.path.exists(gen_directory):\n",
        "    print(\"Create directory: \" + gen_directory)\n",
        "    !mkdir {gen_directory}\n",
        "file_cnt=0\n",
        "try:\n",
        "    for dirname in os.listdir(il_data_path):\n",
        "        date_path=il_data_path+'/'+ dirname + '/'\n",
        "        for filename in os.listdir(date_path):\n",
        "            if filename[:6]=='train_':\n",
        "                train_name= 'train_gen.jsonl'\n",
        "                print('Renaming '+ filename+ ' to ' + train_name)\n",
        "                !mv {date_path+filename} {gen_directory+train_name}\n",
        "                file_cnt+=1\n",
        "            elif filename[:5]=='test_':\n",
        "                test_name= 'test_gen.jsonl'\n",
        "                print('Renaming '+ filename+ ' to ' + test_name)\n",
        "                !mv {date_path+filename} {gen_directory+test_name}\n",
        "                file_cnt+=1\n",
        "    if file_cnt < 2:\n",
        "        print(\"ERROR: train_gen.jsonl and/or test.jsonl not created\")\n",
        "    elif os.path.getsize(gen_directory+train_name) == 0:\n",
        "        print(\"ERROR: train_gen.jsonl file is empty\")\n",
        "    elif os.path.getsize(gen_directory+test_name) == 0:\n",
        "        print(\"ERROR: test_gen.jsonl file is empty\")\n",
        "    else:\n",
        "        print(\"Training and test files successfully created in: \" + gen_directory)\n",
        "except:\n",
        "    print(\"Error running ilab generate, no synthetic data generated\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0B_39Sqvxgj"
      },
      "source": [
        "### 2.5 Show examples of generated data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLqnVW7rvxgj"
      },
      "outputs": [],
      "source": [
        "print(\"2.4.3 Show examples of generated data\")\n",
        "for filename in os.listdir(gen_directory):\n",
        "    if filename[:9]=='train_gen':\n",
        "        with open(gen_directory+filename, 'r') as syn_file:\n",
        "            cnt=0\n",
        "            for line_number, line in enumerate(syn_file):\n",
        "                if cnt >= 8:\n",
        "                    break\n",
        "                jsonLine= json.loads(line)\n",
        "                syn_user=jsonLine[\"user\"]\n",
        "                syn_assist=jsonLine[\"assistant\"]\n",
        "                #Remove \"Answer:\" and \"Response:\" from answers for displaying\n",
        "                if syn_user[:10]==\"Question: \":\n",
        "                    syn_user=syn_user[10:]\n",
        "                if syn_assist[:8]==\"Answer: \":\n",
        "                    syn_assist=syn_assist[8:]\n",
        "                cnt+=1\n",
        "                print(\"\\nQuestion: \"+syn_user+\"\\nAnswer: \"+syn_assist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvTYe8oHdm13",
        "tags": []
      },
      "source": [
        "## Section 3. Training with InstructLab\n",
        "\n",
        "### 3.1 Select the model training pipeline\n",
        "\n",
        "InstructLab has three primary model training pipelines: simple, full (default), and accelerated. For all of the models, the training time can be limited by adjusting the num_epoch paramater. The maximum number of epochs for running the InstructLab end-to-end workflow is 10.\n",
        "\n",
        "#### **Simple pipeline**\n",
        "\n",
        "The simple pipeline uses an SFT Trainer on Linux and MLX on MacOS. This type of training takes roughly an hour and produces the lowest fidelity model but should indicate if your data is being picked up by the training process. The simple pipeline only works with Merlinite 7b Lab as the teacher model. For this Linux system, the trained model is saved in the models directory as ggml-model-f16.gguf.\n",
        "\n",
        "The command form is:\n",
        "\n",
        "    ilab model train --pipeline simple\n",
        "\n",
        "**Note:** This process will take a little while to complete (time can vary based on hardware and output of ilab data generate but on the order of 5 to 15 minutes)\n",
        "\n",
        "#### **Accelerated pipeline**\n",
        "\n",
        "The accelerated uses the instructlab-training library which supports GPU accelerated and distributed training. The full loop and data processing functions are either pulled directly from or based off of the work in this library. For the accelerated pipeline, the models are saved in the ~/.local/share/instructlab/checkpoints directory. The instructlab command \"ilab model evaluate\" can be used to choose the best one. Training is support for GPU acceleration with Nvidia CUDA or AMD ROCm. Please see the GPU acceleration documentation for more details. At present, hardware acceleration requires a data center GPU or high-end consumer GPU with at least 18 GB free memory.\n",
        "\n",
        "The command form is:\n",
        "\n",
        "    ilab model train --pipeline accelerated --device cuda --data-path <path-to-sdg-data>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UrXh9-nvxgj"
      },
      "outputs": [],
      "source": [
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "torch.cuda.empty_cache()\n",
        "print(\"Select to Continue or to Train the model\")\n",
        "display(train_pipe)\n",
        "display(epoch)\n",
        "display(it)\n",
        "print(\"After choosing your training options, please select and run the following cell\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUPA7SpTvxgj"
      },
      "source": [
        "### 3.2 Run the model training\n",
        "\n",
        "Model training can take 30 minutes or more for 1 epoch and 1 iteration and takes 1 hour for the default paramter values. This minimal training could be used for testing the generation and training for a new set of data.\n",
        "\n",
        "To produce a higher quality model, more epochs and iterations are needed for refining the model. This will require a proportionally longer time to train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcIjCQUjvxgj",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "data_path=\"data/\"+ use_case+\"/ilab_generated/\"\n",
        "train_data=data_path+\"train_gen.jsonl\"\n",
        "model_path=\"models/instructlab/granite-7b-lab\"\n",
        "##model_path='/root/.cache/instructlab/models/instructlab/granite-7b-lab'\n",
        "trained_model_path=\"data/\"+ use_case+\"/new_model/\"\n",
        "\n",
        "##'Simple (Fast)', 'Accelerated GPU'\n",
        "file_cnt=0\n",
        "for filename in os.listdir(data_path):\n",
        "    if filename[:15]=='train_gen.jsonl': file_cnt+=1\n",
        "    elif filename[:14]=='test_gen.jsonl': file_cnt+=1\n",
        "if file_cnt < 2 or os.path.getsize(gen_directory+train_name) < 5 or os.path.getsize(gen_directory+test_name) < 5:\n",
        "    print(\"ERROR: train_gen.jsonl and/or test.jsonl are not present or too small\")\n",
        "\n",
        "if not os.path.exists(trained_model_path):\n",
        "    print(\"Create directory: \" + trained_model_path)\n",
        "    !mkdir {trained_model_path}\n",
        "ep=int(epoch.value)\n",
        "its=int(it.value)\n",
        "if train_pipe.value=='Simple with GPU':\n",
        "    print(\"Train with simple pipeline with a GPU\")\n",
        "    shell_command = f\"ilab model train --pipeline simple --device cuda --model-path {model_path} --data-path {data_path} --num-epochs {ep} --iters {its}\"\n",
        "elif train_pipe.value=='Accelerated GPU':\n",
        "    print(\"Train accelerated with a GPU\")\n",
        "    #shell_command = f\"ilab model train --pipeline accelerated --device cuda --model-path {model_path} --data-path {train_data} --num-epochs {ep} --iters {its}\"\n",
        "    shell_command = f\"ilab -v -v model train --pipeline accelerated --device cuda --model-path {'/content/ilab/'+model_path} --data-path {'/content/ilab/'+train_data}\"\n",
        "\n",
        "print(\"Running: !\"+shell_command)\n",
        "!{shell_command}\n",
        "if train_pipe.value=='Accelerated GPU':\n",
        "    print(\"Run ilab model evaluate\")\n",
        "    !ilab model evaluate --benchmark mmlu\n",
        "#Move the model to the use_case/new_model directory\n",
        "print(\"Moving the trained model to the directory: \"+trained_model_path)\n",
        "!mv /root/.local/share/instructlab/checkpoints/ggml-model-f16.gguf {trained_model_path}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SogwbM1vxgk",
        "tags": []
      },
      "source": [
        "# Section 4. Inferencing with the Model\n",
        "\n",
        "You have now completed InstructLab training. You can run this section to ask questions to both the base and InstructLab trained models and to compare answers.\n",
        "\n",
        "This third notebook section showcases the generation of synthetic data utilizing InstructLab. It subsequently demonstrates how a large language model (LLM) can be effectively trained on this synthetic dataset. In current notebook, Both the pre-trained LLM and the LLM trained on the generated synthetic data are evaluated against a predefined set of questions to assess their respective performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWNj4u9Vvxgl",
        "tags": []
      },
      "source": [
        "<a id=\"IL3_3\"></a>\n",
        "## Run Interactive Q&A Session with Base and Trained Models to Evaluate Performance\n",
        "\n",
        "Run both base and trained models to compare results with interactive questions and and answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoD1Ngrv7ni3"
      },
      "outputs": [],
      "source": [
        "print(\"Do you want to run interactive question on the base and trained models?\")\n",
        "display(questions)\n",
        "print(\"After making your choice, please select and run the following cell\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AK5l-lIE7oPF"
      },
      "source": [
        "The following are sample questions derived from the data used to generate synthetic data, which was then employed to train the language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVp-BeByvxgl"
      },
      "outputs": [],
      "source": [
        "# Define a Function to Perform Inference on Base and Trained Models\n",
        "def model_inference(base_model_path, trained_model_path):\n",
        "    _DEFAULT_TEMPLATE = \"\"\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
        "\n",
        "    Current conversation:\n",
        "    Human: {input}\n",
        "    AI:\"\"\"\n",
        "    base_llm = LlamaCpp(model_path=base_model_path,\n",
        "                   verbose=False,\n",
        "                   n_gpu_layers=25,\n",
        "                   max_tokens=90,\n",
        "                   temperature=0,\n",
        "                   top_k=1\n",
        "                  )\n",
        "    trained_llm = LlamaCpp(model_path=trained_model_path,\n",
        "                   verbose=False,\n",
        "                   n_gpu_layers=25,\n",
        "                   max_tokens=90,\n",
        "                   temperature=0,\n",
        "                   top_k=1\n",
        "                  )\n",
        "    PROMPT = PromptTemplate( input_variables=[\"input\"],\n",
        "                            template=_DEFAULT_TEMPLATE\n",
        "                            )\n",
        "    chain1 = PROMPT | base_llm | StrOutputParser()\n",
        "    chain2 = PROMPT | trained_llm | StrOutputParser()\n",
        "    while True:\n",
        "        question = input(\"Ask me a question (type 'exit' to end): \")\n",
        "        if question.lower() == 'exit':\n",
        "            print(\"Exiting this Q&A session.\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"You asked: \", question)\n",
        "            answer1 = chain1.invoke(question)\n",
        "            answer1= answer1.split('Human',1)[0]\n",
        "            print (\"Base Model Answer: \",answer1)\n",
        "            answer2 = chain2.invoke(question)\n",
        "            answer2= answer2.split('Human',1)[0]\n",
        "            print (\"Trained Model Answer: \",answer2)\n",
        "\n",
        "##Display Sample Questions\n",
        "base_model = notebook_dir +\"/models/granite-7b-lab-Q4_K_M.gguf\"\n",
        "trained_model = trained_model_path + \"ggml-model-f16.gguf\"\n",
        "if questions.value=='Yes':\n",
        "  with open(notebook_dir+'/data/' + use_case + '/questions.txt') as f:\n",
        "      for line in f.readlines():\n",
        "          display(widgets.HTML(Norm+line))\n",
        "  print(\"Processing may take several minutes on the first run...\")\n",
        "  model_inference(base_model, trained_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5l5m7L_LDV1"
      },
      "source": [
        "# Section 5. Download the Trained Model\n",
        " Now that we have a model trained on our dataset, we can download the trained model for futher testing and use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kx65h6u17SW_"
      },
      "outputs": [],
      "source": [
        "print(\"Do you want to download the trained model to your local machine?\")\n",
        "display(download)\n",
        "print(\"After making your selection, please select and run the following cell\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwwZ4P6n7TDu"
      },
      "source": [
        "Select and run the next cell to download if selected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZsjOChUK7gj"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "if download.value=='Yes':\n",
        "  files.download(trained_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K4lhpzTvxgl",
        "tags": []
      },
      "source": [
        "<a id=\"IL3_conclusion\"></a>\n",
        "# Conclusion\n",
        "\n",
        "This notebook demonstrated utilizing InstructLab for introducing datasets, data generation, model training, and model creation. This notebook produced an InstructLab trained model that was available for inferecing and downloading."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63hBXXa0vxgl",
        "tags": []
      },
      "source": [
        "<a id=\"IL3_learn\"></a>\n",
        "# Learn More\n",
        "\n",
        "InstructLab uses a novel synthetic data-based alignment tuning method for Large Language Models introduced in this [paper](https://arxiv.org/abs/2403.01081).\n",
        "\n",
        "This notebook is based on the InstructLab CLI repository available [here](https://github.com/instructlab/instructlab).\n",
        "\n",
        "Contact us by email to ask questions, discuss potential use cases, or schedule a technical deep dive. The contact email is IBM.Research.JupyterLab@ibm.com."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poB7nDmcvxgl"
      },
      "source": [
        "© 2025 IBM Corporation"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "57f5e5ba598947a1aee2c57f44ebceec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ToggleButtonsModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ToggleButtonsModel",
            "_options_labels": [
              "Simple",
              "Full with GPU"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ToggleButtonsView",
            "button_style": "",
            "description": "Processing:",
            "description_tooltip": null,
            "disabled": false,
            "icons": [],
            "index": 0,
            "layout": "IPY_MODEL_74d7ca3c5f784184b18207d5771a5ef3",
            "style": "IPY_MODEL_cd1f8b1a24e04357b08f52684e4548a0",
            "tooltips": []
          }
        },
        "74d7ca3c5f784184b18207d5771a5ef3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd1f8b1a24e04357b08f52684e4548a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ToggleButtonsStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ToggleButtonsStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_width": "auto",
            "description_width": "",
            "font_weight": ""
          }
        },
        "19b65567f6d94183bf83daad67717d6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ToggleButtonsModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ToggleButtonsModel",
            "_options_labels": [
              "Default (>450)",
              ">15",
              ">50",
              ">200",
              ">500",
              ">1000"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ToggleButtonsView",
            "button_style": "",
            "description": "# of QNAs:",
            "description_tooltip": null,
            "disabled": false,
            "icons": [],
            "index": 0,
            "layout": "IPY_MODEL_b677242cc4a14a38a5b4cbac8cc20ed6",
            "style": "IPY_MODEL_1452a8ed170c49e9bd09fdfb71f294d1",
            "tooltips": []
          }
        },
        "b677242cc4a14a38a5b4cbac8cc20ed6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1452a8ed170c49e9bd09fdfb71f294d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ToggleButtonsStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ToggleButtonsStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_width": "auto",
            "description_width": "",
            "font_weight": ""
          }
        },
        "def5a6389bae44b9a8c5b803611aaf56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ToggleButtonsModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ToggleButtonsModel",
            "_options_labels": [
              "1",
              "2",
              "3",
              "4",
              "5",
              "10",
              "15"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ToggleButtonsView",
            "button_style": "",
            "description": "Epochs:",
            "description_tooltip": null,
            "disabled": false,
            "icons": [],
            "index": 2,
            "layout": "IPY_MODEL_c493ff0794ec4e838f5c9dcaf0daf978",
            "style": "IPY_MODEL_78d76c42014844aaae51d19751db7a43",
            "tooltips": []
          }
        },
        "c493ff0794ec4e838f5c9dcaf0daf978": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78d76c42014844aaae51d19751db7a43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ToggleButtonsStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ToggleButtonsStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_width": "auto",
            "description_width": "",
            "font_weight": ""
          }
        },
        "787427a67bc542229610bd8707dbcee6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ToggleButtonsModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ToggleButtonsModel",
            "_options_labels": [
              "1",
              "3",
              "5",
              "10",
              "20",
              "50",
              "100",
              "200"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ToggleButtonsView",
            "button_style": "",
            "description": "Iterations:",
            "description_tooltip": null,
            "disabled": false,
            "icons": [],
            "index": 2,
            "layout": "IPY_MODEL_f7dc20f8da87426cb00e35cd8e554c58",
            "style": "IPY_MODEL_d8b32b5e04734512899fdf2852b8b585",
            "tooltips": []
          }
        },
        "f7dc20f8da87426cb00e35cd8e554c58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8b32b5e04734512899fdf2852b8b585": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ToggleButtonsStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ToggleButtonsStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_width": "auto",
            "description_width": "",
            "font_weight": ""
          }
        },
        "54ef6b453a14499cab3afb8b54f566df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ToggleButtonsModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ToggleButtonsModel",
            "_options_labels": [
              "Yes",
              "No"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ToggleButtonsView",
            "button_style": "",
            "description": "Live Q&A:",
            "description_tooltip": null,
            "disabled": false,
            "icons": [],
            "index": 0,
            "layout": "IPY_MODEL_03aa70a8d4c74172bf894021614d3d74",
            "style": "IPY_MODEL_6630967c096c4a74b5c0afd5056520dc",
            "tooltips": []
          }
        },
        "03aa70a8d4c74172bf894021614d3d74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6630967c096c4a74b5c0afd5056520dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ToggleButtonsStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ToggleButtonsStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_width": "auto",
            "description_width": "",
            "font_weight": ""
          }
        },
        "b1b0514af67e4d2ca64704069932a337": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ToggleButtonsModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ToggleButtonsModel",
            "_options_labels": [
              "Yes",
              "No"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ToggleButtonsView",
            "button_style": "",
            "description": "Download:",
            "description_tooltip": null,
            "disabled": false,
            "icons": [],
            "index": 1,
            "layout": "IPY_MODEL_001065ea967d437ab042e77be47c3fae",
            "style": "IPY_MODEL_980ac5b3932f445f97071174eecc5741",
            "tooltips": []
          }
        },
        "001065ea967d437ab042e77be47c3fae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "980ac5b3932f445f97071174eecc5741": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ToggleButtonsStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ToggleButtonsStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_width": "auto",
            "description_width": "",
            "font_weight": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}